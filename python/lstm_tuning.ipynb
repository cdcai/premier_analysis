{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\r\n",
    "import os\r\n",
    "import pickle as pkl\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tensorflow.keras as keras\r\n",
    "import kerastuner\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.utils import compute_class_weight\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "\r\n",
    "from tools import keras as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # GLOBALS   \r\n",
    "TIME_SEQ = 225\r\n",
    "TARGET = \"multi_class\"\r\n",
    "BATCH_SIZE = 32\r\n",
    "EPOCHS = 25\r\n",
    "HB_EPOCHS = 5\r\n",
    "MAX_TRIALS = 500\r\n",
    "TEST_SPLIT = 0.2\r\n",
    "VAL_SPLIT = 0.1\r\n",
    "RAND = 2021\r\n",
    "TB_UPDATE_FREQ = 200\r\n",
    "WEIGHTED_LOSS = False\r\n",
    "\r\n",
    "# Paths\r\n",
    "# BUG: This use to be a cool hack to alway return the root dir\r\n",
    "# of the repo, but that sometimes fails, so just set your PWD here\r\n",
    "# or leave as an empty string if that's where this is running.\r\n",
    "# all paths to output/ and data/ are constructed relative to that\r\n",
    "pwd = \"\"\r\n",
    "\r\n",
    "output_dir = os.path.abspath(os.path.join(pwd, \"..\", \"output\"))\r\n",
    "data_dir = os.path.abspath(os.path.join(pwd, \"..\", \"data\", \"data\"))\r\n",
    "tensorboard_dir = os.path.abspath(\r\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\r\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\r\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\r\n",
    "\r\n",
    "# Create analysis dir if it doesn't exist\r\n",
    "os.makedirs(stats_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "with open(os.path.join(pkl_dir, TARGET + \"_trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "    inputs = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\n",
    "    all_feats = pkl.load(f)\n",
    "\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "    demog_lookup = pkl.load(f)\n",
    "\n",
    "# Determining number of vocab entries\n",
    "N_VOCAB = len(vocab) + 1\n",
    "N_DEMOG = len(demog_lookup) + 1\n",
    "MAX_DEMOG = max(len(x) for _, x, _ in inputs)\n",
    "N_CLASS = max(x for _, _, x in inputs) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metrics and callbacks\r\n",
    "callbacks = [\r\n",
    "    TensorBoard(\r\n",
    "    log_dir=os.path.join(tensorboard_dir, \"lstm_hp_tune_tb\", \"\"),\r\n",
    "    histogram_freq=1,\r\n",
    "    profile_batch=0,\r\n",
    "    write_graph=False,\r\n",
    "    update_freq=TB_UPDATE_FREQ\r\n",
    "    ),\r\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\r\n",
    "                                min_delta=0,\r\n",
    "                                patience=3,\r\n",
    "                                restore_best_weights=True,\r\n",
    "                                mode=\"min\")\r\n",
    "]\r\n",
    "\r\n",
    "# Create some metrics\r\n",
    "metrics = [\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), name=\"ROC-AUC\"),\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), curve=\"PR\", name=\"PR-AUC\")\r\n",
    "]\r\n",
    "\r\n",
    "# Define loss function\r\n",
    "# NOTE: We were experimenting with focal loss at one point, maybe we can try that again at some point\r\n",
    "# loss_fn = keras.losses.categorical_crossentropy if TARGET == \"multi_class\" else keras.losses.binary_crossentropy\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTV\n",
    "# Splitting the data\n",
    "train, test = train_test_split(\n",
    "    range(len(inputs)),\n",
    "    test_size=TEST_SPLIT,\n",
    "    stratify=[labs for _, _, labs in inputs],\n",
    "    random_state=RAND)\n",
    "\n",
    "train, validation = train_test_split(\n",
    "    train,\n",
    "    test_size=VAL_SPLIT,\n",
    "    stratify=[samp[2] for i, samp in enumerate(inputs) if i in train],\n",
    "    random_state=RAND)\n",
    "\n",
    "train_gen = tk.create_ragged_data_gen(\n",
    "    [inputs[samp] for samp in train],\n",
    "    max_demog=MAX_DEMOG,\n",
    "    epochs=EPOCHS,\n",
    "    multiclass=N_CLASS > 2,\n",
    "    random_seed=RAND,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_gen = tk.create_ragged_data_gen(\n",
    "    [inputs[samp] for samp in validation],\n",
    "    max_demog=MAX_DEMOG,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=False,\n",
    "    multiclass=N_CLASS > 2,\n",
    "    random_seed=RAND,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# NOTE: don't shuffle test data\n",
    "test_gen = tk.create_ragged_data_gen([inputs[samp] for samp in test],\n",
    "                                        max_demog=MAX_DEMOG,\n",
    "                                        epochs=1,\n",
    "                                        multiclass=N_CLASS > 2,\n",
    "                                        shuffle=False,\n",
    "                                        random_seed=RAND,\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "# %% Compute steps-per-epoch\n",
    "# NOTE: Sometimes it can't determine this properly from tf.data\n",
    "STEPS_PER_EPOCH = np.ceil(len(train) / BATCH_SIZE)\n",
    "VALID_STEPS_PER_EPOCH = np.ceil(len(validation) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique([labs for _, _, labs in inputs]).tolist()\n",
    "\n",
    "if WEIGHTED_LOSS:\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=classes,\n",
    "        y=[labs for _, _, labs in inputs],\n",
    "    )\n",
    "\n",
    "    class_weights = dict(zip(classes, class_weights))\n",
    "\n",
    "    print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_model = tk.LSTMHyper(\r\n",
    "    vocab_size = N_VOCAB,\r\n",
    "    metrics = metrics,\r\n",
    "    n_classes = N_CLASS,\r\n",
    "    n_demog = N_DEMOG,\r\n",
    "    n_demog_bags=MAX_DEMOG\r\n",
    ")\r\n",
    "\r\n",
    "tuner = kerastuner.tuners.HyperBand(\r\n",
    "    hyper_model,\r\n",
    "    max_epochs=EPOCHS,\r\n",
    "    hyperband_iterations=HB_EPOCHS,\r\n",
    "    objective=\"val_loss\",\r\n",
    "    project_name=\"lstm_hp_tune\",\r\n",
    "    # NOTE: This could be in output as well if we don't want to track/version it\r\n",
    "    directory=tensorboard_dir\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Announce the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_gen,\n",
    "             validation_data=validation_gen,\n",
    "             epochs=EPOCHS,\n",
    "             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "             validation_steps=VALID_STEPS_PER_EPOCH,\n",
    "             callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the best model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(os.path.join(tensorboard_dir, \"best\", \"lstm\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8db72918be02bea17b92dedfee6039df86a0921377fa1138da015447d595378"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2  ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "89d31c033bf313603ee1de07f165bedecb9a1c2d7c2ff2b104ae0fae591794dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}