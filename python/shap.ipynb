{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Shap to do some vis on the D1 DAN model\r\n",
    "Sean Browning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lib ==========\r\n",
    "import os\r\n",
    "import pickle as pkl\r\n",
    "import shap\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow.keras as keras\r\n",
    "import tensorflow_addons as tfa\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.utils.class_weight import compute_class_weight\r\n",
    "\r\n",
    "import tools.analysis as ta\r\n",
    "import tools.preprocessing as tp\r\n",
    "import tools.keras as tk\r\n",
    "from scipy.sparse import lil_matrix\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\r\n",
    "from sklearn.linear_model import (Lasso, LogisticRegression, Ridge,\r\n",
    "                                  SGDClassifier)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GLOBALS ======\r\n",
    "OUTCOME = \"misa_pt\"\r\n",
    "DAY_ONE_ONLY = True\r\n",
    "WEIGHTED_LOSS = False\r\n",
    "TEST_SPLIT = 0.2\r\n",
    "VAL_SPLIT = 0.1\r\n",
    "RAND = 2021\r\n",
    "TB_UPDATE_FREQ = 100\r\n",
    "MOD_NAME = \"dan\"\r\n",
    "TIME_SEQ = 225\r\n",
    "BATCH_SIZE = 128\r\n",
    "EPOCHS = 20\r\n",
    "\r\n",
    "# === DIRS ================\r\n",
    "\r\n",
    "pwd = \"C:/Users/blues/work/premier_analysis/python\"\r\n",
    "\r\n",
    "# If no args are passed to overwrite these values, use repo structure to construct\r\n",
    "data_dir = os.path.abspath(os.path.join(pwd, \"..\", \"data\", \"data\", \"\"))\r\n",
    "output_dir = os.path.abspath(os.path.join(pwd, \"..\", \"output\", \"\"))\r\n",
    "\r\n",
    "tensorboard_dir = os.path.abspath(\r\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\r\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\r\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\r\n",
    "probs_dir = os.path.join(stats_dir, \"probs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\r\n",
    "with open(os.path.join(pkl_dir, OUTCOME + \"_trimmed_seqs.pkl\"), \"rb\") as f:\r\n",
    "    inputs = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\r\n",
    "    vocab = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\r\n",
    "    all_feats = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\r\n",
    "    demog_lookup = pkl.load(f)\r\n",
    "    demog_lookup = {k: v for v, k in demog_lookup.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the inputs and labels\r\n",
    "features = [t[0] for t in inputs]\r\n",
    "demog = [t[1] for t in inputs]\r\n",
    "labels = [t[2] for t in inputs]\r\n",
    "\r\n",
    "# Counts to use for loops and stuff\r\n",
    "n_patients = len(features)\r\n",
    "n_features = np.max(list(vocab.keys()))\r\n",
    "n_classes = len(np.unique(labels))\r\n",
    "binary = n_classes <= 2\r\n",
    "\r\n",
    "# Converting the labels to an array\r\n",
    "y = np.array(labels, dtype=np.uint8)\r\n",
    "\r\n",
    "# Optionally limiting the features to only those from the first day\r\n",
    "# of the actual COVID visit\r\n",
    "if DAY_ONE_ONLY:\r\n",
    "    features = [l[-1] for l in features]\r\n",
    "else:\r\n",
    "    features = [tp.flatten(l) for l in features]\r\n",
    "\r\n",
    "# Optionally mixing in the demographic features\r\n",
    "new_demog = [[i + n_features for i in l] for l in demog]\r\n",
    "features = [features[i] + new_demog[i] for i in range(n_patients)]\r\n",
    "demog_vocab = {k + n_features: v for k, v in demog_lookup.items()}\r\n",
    "vocab.update(demog_vocab)\r\n",
    "n_features = np.max([np.max(l) for l in features])\r\n",
    "all_feats.update({v: v for k, v in demog_lookup.items()})\r\n",
    "\r\n",
    "# Converting the features to a sparse matrix\r\n",
    "mat = lil_matrix((n_patients, n_features + 1))\r\n",
    "for row, cols in enumerate(features):\r\n",
    "    mat[row, cols] = 1\r\n",
    "\r\n",
    "# Converting to csr because the internet said it would be faster\r\n",
    "X = mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\r\n",
    "train, test = train_test_split(range(n_patients),\r\n",
    "                                test_size=TEST_SPLIT,\r\n",
    "                                stratify=y,\r\n",
    "                                random_state=RAND)\r\n",
    "\r\n",
    "train, val = train_test_split(train,\r\n",
    "                                test_size=VAL_SPLIT,\r\n",
    "                                stratify=y[train],\r\n",
    "                                random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = RandomForestClassifier(n_estimators=500, n_jobs=-1)\r\n",
    "mod.fit(X[train], y[train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Shapley\r\n",
    "BUG: DeepExplainer is failing with Embedding layers. Not sure a workaround other than to use a basic kernel.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to take a sample of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a list of samples we'll visualize\r\n",
    "shap_train_sample_idx = np.random.choice(train, 100, replace=False)\r\n",
    "shap_test_sample_idx = np.random.choice(test, 100, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our explainer\r\n",
    "Generating shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain = shap.Explainer(mod, X[shap_train_sample_idx].toarray())\r\n",
    "shap_vals = explain.shap_values(X[shap_test_sample_idx].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-transforming integer sequences to text names for vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence\r\n",
    "inv_vocab = {k: v for v, k in vocab.items()}\r\n",
    "inv_vocab_df = pd.DataFrame.from_dict(inv_vocab, \"index\", columns=[\"idx\"])\r\n",
    "all_feats_df = pd.DataFrame.from_dict(all_feats, \"index\", columns = [\"val_txt\"])\r\n",
    "\r\n",
    "vocab_df = inv_vocab_df.join(all_feats_df, how = \"left\").reset_index()\r\n",
    "vocab_df.set_index(\"idx\", inplace=True)\r\n",
    "\r\n",
    "vocab_df[\"val_txt\"] = np.where(vocab_df[\"val_txt\"].isnull(), vocab_df[\"index\"], vocab_df[\"val_txt\"])\r\n",
    "\r\n",
    "vocab_df = vocab_df.sort_index()\r\n",
    "\r\n",
    "feat_names = vocab_df[\"val_txt\"].tolist()\r\n",
    "feat_names.insert(0, \"OOV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\r\n",
    "expected_val = explain.expected_value\r\n",
    "print(f\"Explainer expected value: {expected_val}\")\r\n",
    "\r\n",
    "shap.decision_plot(\r\n",
    "    expected_val[0],\r\n",
    "    shap_vals[0],\r\n",
    "    link=\"logit\",\r\n",
    "    feature_names = feat_names,\r\n",
    "    ignore_warnings=True\r\n",
    ")\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89d31c033bf313603ee1de07f165bedecb9a1c2d7c2ff2b104ae0fae591794dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}