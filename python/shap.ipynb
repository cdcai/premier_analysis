{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Shap to do some vis on the D1 DAN model\r\n",
    "Sean Browning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lib ==========\r\n",
    "import os\r\n",
    "import pickle as pkl\r\n",
    "import shap\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow.keras as keras\r\n",
    "import tensorflow_addons as tfa\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.utils.class_weight import compute_class_weight\r\n",
    "from tensorflow.keras.callbacks import TensorBoard\r\n",
    "\r\n",
    "import tools.analysis as ta\r\n",
    "import tools.preprocessing as tp\r\n",
    "import tools.keras as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GLOBALS ======\r\n",
    "OUTCOME = \"misa_pt\"\r\n",
    "DAY_ONE_ONLY = True\r\n",
    "WEIGHTED_LOSS = False\r\n",
    "TEST_SPLIT = 0.2\r\n",
    "VAL_SPLIT = 0.1\r\n",
    "RAND = 2021\r\n",
    "TB_UPDATE_FREQ = 100\r\n",
    "MOD_NAME = \"dan\"\r\n",
    "TIME_SEQ = 225\r\n",
    "BATCH_SIZE = 128\r\n",
    "EPOCHS = 20\r\n",
    "\r\n",
    "# === DIRS ================\r\n",
    "\r\n",
    "pwd = globals()['_dh'][0]\r\n",
    "\r\n",
    "# If no args are passed to overwrite these values, use repo structure to construct\r\n",
    "data_dir = os.path.abspath(os.path.join(pwd, \"..\", \"data\", \"data\", \"\"))\r\n",
    "output_dir = os.path.abspath(os.path.join(pwd, \"..\", \"output\", \"\"))\r\n",
    "\r\n",
    "tensorboard_dir = os.path.abspath(\r\n",
    "    os.path.join(data_dir, \"..\", \"model_checkpoints\"))\r\n",
    "pkl_dir = os.path.join(output_dir, \"pkl\")\r\n",
    "stats_dir = os.path.join(output_dir, \"analysis\")\r\n",
    "probs_dir = os.path.join(stats_dir, \"probs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\r\n",
    "with open(os.path.join(pkl_dir, OUTCOME + \"_trimmed_seqs.pkl\"), \"rb\") as f:\r\n",
    "    inputs = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\r\n",
    "    vocab = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"feature_lookup.pkl\"), \"rb\") as f:\r\n",
    "    all_feats = pkl.load(f)\r\n",
    "\r\n",
    "with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\r\n",
    "    demog_lookup = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining number of vocab entries\r\n",
    "N_VOCAB = len(vocab) + 1\r\n",
    "N_DEMOG = len(demog_lookup) + 1\r\n",
    "MAX_DEMOG = max(len(x) for _, x, _ in inputs)\r\n",
    "N_CLASS = max(x for _, _, x in inputs) + 1\r\n",
    "\r\n",
    "# Setting y here so it's stable\r\n",
    "y = np.array([l[2] for l in inputs])\r\n",
    "\r\n",
    "# Create some metrics\r\n",
    "metrics = [\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), name=\"ROC-AUC\"),\r\n",
    "    keras.metrics.AUC(num_thresholds=int(1e5), curve=\"PR\", name=\"PR-AUC\"),\r\n",
    "    tfa.metrics.F1Score(num_classes=N_CLASS if N_CLASS > 2 else 1, average=\"weighted\")\r\n",
    "]\r\n",
    "\r\n",
    "if OUTCOME == 'multi_class':\r\n",
    "    loss_fn = keras.losses.categorical_crossentropy\r\n",
    "else:\r\n",
    "    loss_fn = keras.losses.binary_crossentropy\r\n",
    "\r\n",
    "callbacks = [\r\n",
    "    # Create early stopping callback\r\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\r\n",
    "                                    min_delta=0,\r\n",
    "                                    patience=2,\r\n",
    "                                    mode=\"auto\")\r\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\r\n",
    "train, test = train_test_split(range(len(inputs)),\r\n",
    "                                test_size=TEST_SPLIT,\r\n",
    "                                stratify=y,\r\n",
    "                                random_state=RAND)\r\n",
    "\r\n",
    "train, val = train_test_split(train,\r\n",
    "                                test_size=VAL_SPLIT,\r\n",
    "                                stratify=y[train],\r\n",
    "                                random_state=RAND)\r\n",
    "\r\n",
    "# Optional weighting\r\n",
    "if WEIGHTED_LOSS:\r\n",
    "    classes = np.unique(y)\r\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y[train])\r\n",
    "    weight_dict = {c: weights[i] for i, c in enumerate(classes)}\r\n",
    "else:\r\n",
    "    weight_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DAY_ONE_ONLY:\r\n",
    "    # Optionally limiting the features to only those from the first day\r\n",
    "    # of the actual COVID visit\r\n",
    "    features = [l[0][-1] for l in inputs]\r\n",
    "else:\r\n",
    "    features = [tp.flatten(l[0]) for l in inputs]\r\n",
    "\r\n",
    "# Handling demog\r\n",
    "new_demog = [[i + N_VOCAB - 1 for i in l[1]] for l in inputs]\r\n",
    "features = [\r\n",
    "    features[i] + new_demog[i] for i in range(len(features))\r\n",
    "]\r\n",
    "demog_vocab = {v + N_VOCAB - 1: k for k, v in demog_lookup.items()}\r\n",
    "vocab.update(demog_vocab)\r\n",
    "N_VOCAB = np.max([np.max(l) for l in features]) + 1\r\n",
    "\r\n",
    "# Making the variables\r\n",
    "X = keras.preprocessing.sequence.pad_sequences(features,\r\n",
    "                                                maxlen=225,\r\n",
    "                                                padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_CLASS > 2:\r\n",
    "    # We have to pass one-hot labels for model fit, but CLF metrics\r\n",
    "    # will take indices\r\n",
    "    n_values = np.max(y) + 1\r\n",
    "    y_one_hot = np.eye(n_values)[y]\r\n",
    "\r\n",
    "    # Produce DAN model to fit\r\n",
    "    model = tk.DAN(vocab_size=N_VOCAB,\r\n",
    "                    ragged=False,\r\n",
    "                    input_length=TIME_SEQ,\r\n",
    "                    n_classes=N_CLASS)\r\n",
    "\r\n",
    "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metrics)\r\n",
    "\r\n",
    "    model.fit(X[train],\r\n",
    "                y_one_hot[train],\r\n",
    "                batch_size=BATCH_SIZE,\r\n",
    "                epochs=EPOCHS,\r\n",
    "                validation_data=(X[val], y_one_hot[val]),\r\n",
    "                callbacks=callbacks,\r\n",
    "                class_weight=weight_dict)\r\n",
    "\r\n",
    "else:\r\n",
    "    # Produce DAN model to fit\r\n",
    "    model = tk.DAN(vocab_size=N_VOCAB,\r\n",
    "                    ragged=False,\r\n",
    "                    input_length=TIME_SEQ)\r\n",
    "\r\n",
    "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metrics)\r\n",
    "\r\n",
    "    model.fit(X[train],\r\n",
    "                y[train],\r\n",
    "                batch_size=BATCH_SIZE,\r\n",
    "                epochs=EPOCHS,\r\n",
    "                validation_data=(X[val], y[val]),\r\n",
    "                callbacks=callbacks,\r\n",
    "                class_weight=weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce DAN predictions on validation and test sets\r\n",
    "train_probs = model.predict(X[train])\r\n",
    "val_probs = model.predict(X[val])\r\n",
    "test_probs = model.predict(X[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Shapley\r\n",
    "BUG: DeepExplainer is failing with Embedding layers. Not sure a workaround other than to use a basic kernel.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to take a sample of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_sample = X[np.random.choice(train, 100, replace=False)]\r\n",
    "\r\n",
    "# This is a list of samples we'll visualize\r\n",
    "shap_test_sample = X[np.random.choice(test, 10, replace=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\r\n",
    "    return model.predict([x[:,i] for i in range(x.shape[1])]).flatten()\r\n",
    "\r\n",
    "explain = shap.KernelExplainer(f, shap_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shap_vals = explain.shap_values(shap_test_sample, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-transforming integer sequences to text names for vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence\r\n",
    "inv_vocab = {k: v for v, k in vocab.items()}\r\n",
    "inv_vocab_df = pd.DataFrame.from_dict(inv_vocab, \"index\", columns=[\"idx\"])\r\n",
    "all_feats_df = pd.DataFrame.from_dict(all_feats, \"index\", columns = [\"val_txt\"])\r\n",
    "\r\n",
    "vocab_df = inv_vocab_df.join(all_feats_df, how = \"left\").reset_index()\r\n",
    "vocab_df.set_index(\"idx\", inplace=True)\r\n",
    "\r\n",
    "vocab_df[\"val_txt\"] = np.where(vocab_df[\"val_txt\"].isnull(), vocab_df[\"index\"], vocab_df[\"val_txt\"])\r\n",
    "\r\n",
    "vocab_df = vocab_df.sort_index()\r\n",
    "\r\n",
    "feat_names = vocab_df[\"val_txt\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_val = explain.expected_value\r\n",
    "print(f\"Explainer expected value: {expected_val}\")\r\n",
    "\r\n",
    "shap.decision_plot(expected_val, test_shap_vals, feature_names = feat_names, feature_display=range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\r\n",
    "shap.decision_plot(expected_val, test_shap_vals, feature_names = feat_names, feature_display=range(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2  ('venv': venv)",
   "name": "python38264bitvenvvenv11f8ace3faad4dd08d79c36b9c39aaa4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "a8db72918be02bea17b92dedfee6039df86a0921377fa1138da015447d595378"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}