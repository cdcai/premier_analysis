{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Azure AutoML to predict COVID-19 outomes from EHR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import azureml\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from azureml.core import Run, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import ScriptRunConfig, Environment\n",
    "from azureml.core import  Workspace\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ws = Workspace.from_config()\n",
    "#ws = Workspace.get(name=workspace,\n",
    "#                    resource_group=resource_group,\n",
    "#                    subscription_id=subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: cdh-azml-dev-mlw\n",
      "Azure region: eastus\n",
      "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
      "Resource group: CSELS-CDH-DEV\n"
     ]
    }
   ],
   "source": [
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\wsn8\\Code\\premier_analysis\\azure_ml\n",
      "\n",
      "Parent Directory: c:\\Users\\wsn8\\Code\\premier_analysis\n"
     ]
    }
   ],
   "source": [
    "# current working directory\n",
    "pwd = os.getcwd()\n",
    "print(\"Current Directory:\", pwd)\n",
    "  \n",
    "# parent directory\n",
    "parent = os.path.join(pwd, os.pardir)\n",
    "  \n",
    "# prints parent directory\n",
    "print(\"\\nParent Directory:\", os.path.abspath(parent))\n",
    "\n",
    "premier_path = os.path.abspath(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the existing cluster\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "clustername = 'StandardD13v2'\n",
    "is_new_cluster = False\n",
    "try:\n",
    "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
    "    print(\"Find the existing cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Cluster not find - Creating cluster.....\")\n",
    "    is_new_cluster = True\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
    "                                                           max_nodes=2)\n",
    "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
    "\n",
    "aml_compute_cpu.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/eastus/workspaces/d5539876-73f2-429b-9d16-cd4969e1602d/environments/premier_train_baseline_env/versions/4\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220915.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": true,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"premier_train_baseline_env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"default\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"matplotlib\",\n",
       "                        \"pandas\",\n",
       "                        \"argparse\",\n",
       "                        \"joblib\",\n",
       "                        \"scikit-learn\",\n",
       "                        \"azureml-sdk\",\n",
       "                        \"openpyxl\"\n",
       "                    ]\n",
       "                }\n",
       "            ]\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"4\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_train_baseline_env = Environment.from_conda_specification(name='premier_train_baseline_env', file_path='conda_dependencies_baseline.yml')\n",
    "# Specify a CPU base image\n",
    "premier_train_baseline_env.docker.enabled = True\n",
    "premier_train_baseline_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "premier_train_baseline_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Run\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "data_store = ws.get_default_datastore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./automl/baseline_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile ./automl/baseline_preprocessing.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "from azureml.core import Model,Dataset\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def baseline_preprocessing(OUTCOME):\n",
    "\n",
    "    DAY_ONE_ONLY = True\n",
    "    USE_DEMOG = True\n",
    "    TEST_SPLIT = 0.1\n",
    "    VAL_SPLIT = 0.8\n",
    "    RAND = 42\n",
    "\n",
    "    run = Run.get_context()\n",
    "    print(\"run name:\",run.display_name)\n",
    "    print(\"run details:\",run.get_details())\n",
    "    \n",
    "    ws = run.experiment.workspace\n",
    "\n",
    "    data_store = ws.get_default_datastore()\n",
    "\n",
    "    print(\"Creating dataset from Datastore\")\n",
    "    inputs = Dataset.File.from_files(path=data_store.path('output/pkl/trimmed_seqs.pkl'))  \n",
    "    vocab = Dataset.File.from_files(path=data_store.path('output/pkl/all_ftrs_dict.pkl'))\n",
    "    demog_dict = Dataset.File.from_files(path=data_store.path('output/pkl/demog_dict.pkl'))\n",
    "    cohort = Dataset.Tabular.from_delimited_files(path=data_store.path('output/cohort/cohort.csv'))\n",
    "    \n",
    "    pwd = os.path.dirname(__file__)\n",
    "    output_dir = os.path.abspath(os.path.join(pwd,\"output\"))\n",
    "    pkl_dir = os.path.join(output_dir, \"pkl\")\n",
    "    csv_dir = os.path.join(output_dir, \"csv\")\n",
    "\n",
    "    os.makedirs(pkl_dir, exist_ok=True)\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Downloading data from Datastore...\")\n",
    "\n",
    "    inputs.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    vocab.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    demog_dict.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    cohort.to_pandas_dataframe().to_csv(os.path.join(csv_dir,'cohort.csv'))\n",
    "\n",
    "    print(\"Loading var...\")\n",
    "    with open(os.path.join(pkl_dir, \"trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "        inputs = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "        vocab = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "        demog_dict = pkl.load(f)\n",
    "        demog_dict = {k: v for v, k in demog_dict.items()}\n",
    "\n",
    "    \n",
    "    # Separating the inputs and labels\n",
    "    features = [t[0] for t in inputs]\n",
    "    demog = [t[1] for t in inputs]\n",
    "    cohort = pd.read_csv(os.path.join(csv_dir, 'cohort.csv'))\n",
    "    labels = cohort[OUTCOME]\n",
    "\n",
    "    # Counts to use for loops and stuff\n",
    "    n_patients = len(features)\n",
    "    n_features = np.max(list(vocab.keys()))\n",
    "    n_classes = len(np.unique(labels))\n",
    "    binary = n_classes <= 2\n",
    "\n",
    "        # Converting the labels to an array\n",
    "    y = np.array(labels, dtype=np.uint8)\n",
    "\n",
    "    # Optionally limiting the features to only those from the first day\n",
    "    # of the actual COVID visit\n",
    "    if DAY_ONE_ONLY:\n",
    "        features = [l[-1] for l in features]\n",
    "    else:\n",
    "        features = [flatten(l) for l in features]\n",
    "\n",
    "    new_demog = [[i + n_features for i in l] for l in demog]\n",
    "    features = [features[i] + new_demog[i] for i in range(n_patients)]\n",
    "    demog_vocab = {k + n_features: v for k, v in demog_dict.items()}\n",
    "    vocab.update(demog_vocab)\n",
    "    n_features = np.max([np.max(l) for l in features])\n",
    "    # all_feats.update({v: v for k, v in demog_dict.items()})\n",
    "\n",
    "    # Converting the features to a sparse matrix\n",
    "    mat = lil_matrix((n_patients, n_features + 1))\n",
    "    for row, cols in enumerate(features):\n",
    "        mat[row, cols] = 1\n",
    "\n",
    "    # Converting to csr because the internet said it would be faster\n",
    "    print(\"Converting to csr..\")\n",
    "    X = mat.tocsr()\n",
    "\n",
    "    # Splitting the data; 'all' will produce the same test sample\n",
    "    # for every outcome (kinda nice)\n",
    "\n",
    "    STRATIFY = None\n",
    "\n",
    "    strat_var = y\n",
    "    train, test = train_test_split(range(n_patients),\n",
    "                                    test_size=TEST_SPLIT,\n",
    "                                    stratify=strat_var,\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    # Doing a validation split for threshold-picking on binary problems\n",
    "    train, val = train_test_split(train,\n",
    "                                    test_size=VAL_SPLIT,\n",
    "                                    stratify=strat_var[train],\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    from sklearn.decomposition import PCA,TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=2000)\n",
    "    \n",
    "    x_train = svd.fit_transform(X[train])\n",
    "    x_test = svd.transform(X[test])\n",
    "\n",
    "\n",
    "\n",
    "    return  x_train,y[train],x_test,y[test]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"feature\")\n",
    "    parser.add_argument(\"--outcome\",type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    OUTCOME = args.outcome\n",
    "\n",
    "    x_train,y_train,x_test,y_test = baseline_preprocessing(OUTCOME=OUTCOME)\n",
    "\n",
    "    #x_train_df = pd.DataFrame.sparse.from_spmatrix(x_train)\n",
    "    #x_test_df = pd.DataFrame.sparse.from_spmatrix(x_test)\n",
    "\n",
    "    #train_data = pd.concat([x_train_df,pd.DataFrame(y_train)],axis =1)\n",
    "    train_data = pd.DataFrame(x_train)\n",
    "    train_data['class'] = y_train\n",
    "    \n",
    "    print(\"train shape:\",train_data.shape)\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "\n",
    "    data_store = ws.get_default_datastore()\n",
    "    pwd = os.path.dirname(__file__)\n",
    "    feature_dir = os.path.join(pwd,\"output\",\"automl\")\n",
    "    os.makedirs(feature_dir,exist_ok=True)\n",
    "\n",
    "    print(\"Saving train_data...\")\n",
    "    np.savetxt(os.path.join(feature_dir,\"train_data.csv\"), train_data, delimiter=\",\")\n",
    "\n",
    "    dataset_name = f\"train-data-baseline-{OUTCOME}\"\n",
    "    #ds_train = Dataset.Tabular.register_pandas_dataframe(train_data,target=data_store,name=dataset_name,show_progress=True)\n",
    "    data_store.upload(src_dir=feature_dir,target_path=\"output/automl\",overwrite=True,show_progress=True)\n",
    "    \n",
    "    datastore_paths = [(data_store, \"output/automl\")]\n",
    "\n",
    "    print(\"Create dataset...\")\n",
    "    inputs = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "\n",
    "    print(\"Register dataset..\")\n",
    "    inputs.register(name=dataset_name,workspace=ws,create_new_version=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "from azureml.core import Model,Dataset\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def baseline_preprocessing(OUTCOME):\n",
    "\n",
    "    DAY_ONE_ONLY = True\n",
    "    USE_DEMOG = True\n",
    "    TEST_SPLIT = 0.1\n",
    "    VAL_SPLIT = 0.8\n",
    "    RAND = 42\n",
    "\n",
    "    #run = Run.get_context()\n",
    "    #print(\"run name:\",run.display_name)\n",
    "    #print(\"run details:\",run.get_details())\n",
    "    \n",
    "    #ws = run.experiment.workspace\n",
    "\n",
    "    data_store = ws.get_default_datastore()\n",
    "\n",
    "    print(\"Creating dataset from Datastore\")\n",
    "    inputs = Dataset.File.from_files(path=data_store.path('output/pkl/trimmed_seqs.pkl'))  \n",
    "    vocab = Dataset.File.from_files(path=data_store.path('output/pkl/all_ftrs_dict.pkl'))\n",
    "    demog_dict = Dataset.File.from_files(path=data_store.path('output/pkl/demog_dict.pkl'))\n",
    "    cohort = Dataset.Tabular.from_delimited_files(path=data_store.path('output/cohort/cohort.csv'))\n",
    "    \n",
    "    pwd = os.getcwd()\n",
    "    output_dir = os.path.abspath(os.path.join(pwd,\"output\"))\n",
    "    pkl_dir = os.path.join(output_dir, \"pkl\")\n",
    "    csv_dir = os.path.join(output_dir, \"csv\")\n",
    "\n",
    "    os.makedirs(pkl_dir, exist_ok=True)\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Downloading data from Datastore...\")\n",
    "\n",
    "    inputs.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    vocab.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    demog_dict.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    cohort.to_pandas_dataframe().to_csv(os.path.join(csv_dir,'cohort.csv'))\n",
    "\n",
    "    print(\"Loading var...\")\n",
    "    with open(os.path.join(pkl_dir, \"trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "        inputs = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "        vocab = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "        demog_dict = pkl.load(f)\n",
    "        demog_dict = {k: v for v, k in demog_dict.items()}\n",
    "\n",
    "    \n",
    "    # Separating the inputs and labels\n",
    "    features = [t[0] for t in inputs]\n",
    "    demog = [t[1] for t in inputs]\n",
    "    cohort = pd.read_csv(os.path.join(csv_dir, 'cohort.csv'))\n",
    "    labels = cohort[OUTCOME]\n",
    "\n",
    "    # Counts to use for loops and stuff\n",
    "    n_patients = len(features)\n",
    "    n_features = np.max(list(vocab.keys()))\n",
    "    n_classes = len(np.unique(labels))\n",
    "    binary = n_classes <= 2\n",
    "\n",
    "        # Converting the labels to an array\n",
    "    y = np.array(labels, dtype=np.uint8)\n",
    "\n",
    "    # Optionally limiting the features to only those from the first day\n",
    "    # of the actual COVID visit\n",
    "    if DAY_ONE_ONLY:\n",
    "        features = [l[-1] for l in features]\n",
    "    else:\n",
    "        features = [flatten(l) for l in features]\n",
    "\n",
    "    new_demog = [[i + n_features for i in l] for l in demog]\n",
    "    features = [features[i] + new_demog[i] for i in range(n_patients)]\n",
    "    demog_vocab = {k + n_features: v for k, v in demog_dict.items()}\n",
    "    vocab.update(demog_vocab)\n",
    "    n_features = np.max([np.max(l) for l in features])\n",
    "    # all_feats.update({v: v for k, v in demog_dict.items()})\n",
    "\n",
    "    # Converting the features to a sparse matrix\n",
    "    mat = lil_matrix((n_patients, n_features + 1))\n",
    "    for row, cols in enumerate(features):\n",
    "        mat[row, cols] = 1\n",
    "\n",
    "    # Converting to csr because the internet said it would be faster\n",
    "    print(\"Converting to csr..\")\n",
    "    X = mat.tocsr()\n",
    "\n",
    "    # Splitting the data; 'all' will produce the same test sample\n",
    "    # for every outcome (kinda nice)\n",
    "\n",
    "    STRATIFY = None\n",
    "\n",
    "    strat_var = y\n",
    "    train, test = train_test_split(range(n_patients),\n",
    "                                    test_size=TEST_SPLIT,\n",
    "                                    stratify=strat_var,\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    # Doing a validation split for threshold-picking on binary problems\n",
    "    train, val = train_test_split(train,\n",
    "                                    test_size=VAL_SPLIT,\n",
    "                                    stratify=strat_var[train],\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    return  X[train],y[train],X[test],y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from Datastore\n",
      "Downloading data from Datastore...\n",
      "Loading var...\n",
      "Converting to csr..\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "OUTCOME = 'icu'\n",
    "\n",
    "x_train,y_train,x_test,y_test = baseline_preprocessing(OUTCOME=OUTCOME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12161, 44744)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=2000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2000)\n",
    "svd.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12161, 2000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_new = svd.transform(x_train)\n",
    "x_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(x_train_new)\n",
    "train_data['class']=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./output/train/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./output/train\\train_data.csv\n",
      "Uploaded ./output/train\\train_data.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Create dataset...\n",
      "Register dataset..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'output/train')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\",\n",
       "    \"ParseDelimited\",\n",
       "    \"DropColumns\",\n",
       "    \"SetColumnTypes\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"525aa2ba-c266-4d67-91cc-3bb4edf05d77\",\n",
       "    \"name\": \"train-data-baseline-{OUTCOME}\",\n",
       "    \"version\": 1,\n",
       "    \"workspace\": \"Workspace.create(name='cdh-azml-dev-mlw', subscription_id='320d8d57-c87c-4434-827f-59ee7d86687a', resource_group='CSELS-CDH-DEV')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_store.upload(src_dir='./output/train/',target_path=\"output/train\",overwrite=True,show_progress=True)\n",
    "    \n",
    "datastore_paths = [(data_store, \"output/train\")]\n",
    "\n",
    "print(\"Create dataset...\")\n",
    "inputs = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "\n",
    "print(\"Register dataset..\")\n",
    "inputs.register(name=f'train-data-baseline-{OUTCOME}',workspace=ws,create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_feature_processing created\n"
     ]
    }
   ],
   "source": [
    "OUTCOME = 'icu'\n",
    "source_directory ='./automl'\n",
    "job_feature_processing = ScriptRunConfig(\n",
    "                         script=\"baseline_preprocessing.py\", \n",
    "                         arguments=[\"--outcome\",OUTCOME],\n",
    "                         compute_target=aml_compute_cpu, \n",
    "                         environment=premier_train_baseline_env,\n",
    "                         source_directory=source_directory)\n",
    "print(\"job_feature_processing created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit Experiment: Job-feature-preprocess-automl-baseline\n"
     ]
    }
   ],
   "source": [
    "exp_name = f\"Job-feature-preprocess-automl-baseline\"\n",
    "print(\"Submit Experiment:\",exp_name)\n",
    "# Create experiment\n",
    "experiment = Experiment(workspace=ws, name = exp_name)\n",
    "run = experiment.submit(job_feature_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto ML configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# Get the batch dataset for input\n",
    "dataset_name = \"train-data-baseline-{OUTCOME}\"\n",
    "dataset = Dataset.get_by_name(ws, name=dataset_name)\n",
    "#dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME = 'icu'\n",
    "automl_settings = {\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"iteration_timeout_minutes\": 25,\n",
    "    \"max_concurrent_iterations\": 4,\n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"enable_code_generation\": True,\n",
    "}\n",
    "\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "    experiment_timeout_minutes=90,\n",
    "    task='classification',\n",
    "    primary_metric=\"accuracy\",\n",
    "    allowed_models= [\"GradientBoosting\",\"LightGBM\",\"LogisticRegression\",\"SVM\"],\n",
    "    compute_target = aml_compute_cpu,\n",
    "    training_data=dataset,\n",
    "    label_column_name='class',\n",
    "    featurization = 'auto',\n",
    "    n_cross_validations=5,\n",
    "    **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting remote run.\n",
      "No run_configuration provided, running on StandardD13v2 with default configuration\n",
      "Running on remote compute: StandardD13v2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Job-automl-train-baseline-icu</td><td>AutoML_5557f4d6-c0e4-45c4-b0e9-e91bfac70c85</td><td>automl</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/AutoML_5557f4d6-c0e4-45c4-b0e9-e91bfac70c85?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/CSELS-CDH-DEV/workspaces/cdh-azml-dev-mlw&amp;tid=9ce70869-60db-44fd-abe8-d2767077fc8f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "********************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Class balancing detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and all classes are balanced in your training data.\n",
      "              Learn more about imbalanced data: https://aka.ms/AutomatedMLImbalancedData\n",
      "\n",
      "********************************************************************************************\n",
      "\n",
      "TYPE:         Missing feature values imputation\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  No feature missing values were detected in the training data.\n",
      "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "********************************************************************************************\n",
      "\n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n",
      "\n",
      "********************************************************************************************\n",
      "\n",
      "********************************************************************************************\n",
      "ITER: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "********************************************************************************************\n",
      "\n",
      " ITER   PIPELINE                                       DURATION            METRIC      BEST\n",
      "    0   MaxAbsScaler LightGBM                          0:05:42             0.7663    0.7663\n",
      "    1   MaxAbsScaler LightGBM                          0:05:05             0.7627    0.7663\n",
      "    2   RobustScaler LogisticRegression                0:08:36             0.6663    0.7663\n",
      "    3   MaxAbsScaler LightGBM                          0:05:03             0.7551    0.7663\n",
      "    4   StandardScalerWrapper SVM                      0:25:02                nan    0.7663\n",
      "ERROR: {\n",
      "    \"additional_properties\": {},\n",
      "    \"error\": {\n",
      "        \"additional_properties\": {\n",
      "            \"debugInfo\": null\n",
      "        },\n",
      "        \"code\": \"UserError\",\n",
      "        \"severity\": null,\n",
      "        \"message\": \"Iteration timeout reached, skipping execution of the child run. Consider increasing iteration_timeout_minutes.\",\n",
      "        \"message_format\": \"Iteration timeout reached, skipping execution of the child run. Consider increasing iteration_timeout_minutes.\",\n",
      "        \"message_parameters\": {},\n",
      "        \"reference_code\": null,\n",
      "        \"details_uri\": \"https://aka.ms/azureml-run-troubleshooting\",\n",
      "        \"target\": null,\n",
      "        \"details\": [],\n",
      "        \"inner_error\": {\n",
      "            \"additional_properties\": {},\n",
      "            \"code\": \"ResourceExhausted\",\n",
      "            \"inner_error\": {\n",
      "                \"additional_properties\": {},\n",
      "                \"code\": \"Timeout\",\n",
      "                \"inner_error\": {\n",
      "                    \"additional_properties\": {},\n",
      "                    \"code\": \"IterationTimedOut\",\n",
      "                    \"inner_error\": null\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"additional_info\": null\n",
      "    },\n",
      "    \"correlation\": null,\n",
      "    \"environment\": null,\n",
      "    \"location\": null,\n",
      "    \"time\": {},\n",
      "    \"component_name\": null\n",
      "}\n",
      "    5   StandardScalerWrapper LogisticRegression       0:08:46             0.7704    0.7704\n",
      "    6   MaxAbsScaler LogisticRegression                0:08:41             0.7236    0.7704\n",
      "    7   SparseNormalizer LightGBM                      0:05:02             0.7551    0.7704\n",
      "    8   MaxAbsScaler LogisticRegression                0:08:39             0.7132    0.7704\n",
      "    9   StandardScalerWrapper LogisticRegression       0:07:01                nan    0.7704\n",
      "   10                                                  1:34:22                nan    0.7704\n",
      "   11                                                  1:34:21                nan    0.7704\n",
      "   12                                                  1:34:20                nan    0.7704\n",
      "   13                                                  1:34:20                nan    0.7704\n",
      "   14                                                  1:34:20                nan    0.7704\n",
      "   15                                                  1:34:20                nan    0.7704\n",
      "   16                                                  1:34:19                nan    0.7704\n",
      "   17                                                  1:34:18                nan    0.7704\n",
      "   18                                                  1:34:18                nan    0.7704\n",
      "   19                                                  1:34:18                nan    0.7704\n",
      "   20    VotingEnsemble                                0:02:08             0.7738    0.7738\n",
      "   21    StackEnsemble                                 0:02:13             0.7742    0.7742\n"
     ]
    }
   ],
   "source": [
    "# Submit your automl run\n",
    "exp_name = f\"Job-automl-train-baseline-{OUTCOME}\"\n",
    "exp = Experiment(workspace=ws, name=exp_name)\n",
    "automl_run = exp.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('premier_score_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc78c87736aeb3cc28398dbcb4d22314bd77bb9dc1f4148cd78e25835521c0d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
