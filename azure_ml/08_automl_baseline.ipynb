{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Azure AutoML to predict COVID-19 outomes from EHR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import azureml\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from azureml.core import Run, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import ScriptRunConfig, Environment\n",
    "from azureml.core import  Workspace\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"cdh-azml-dev-mlw\"\n",
    "resource_group= 'CSELS-CDH-DEV'\n",
    "subscription_id= \"320d8d57-c87c-4434-827f-59ee7d86687a\"\n",
    "\n",
    "ws = Workspace.get(name=workspace,\n",
    "                    resource_group=resource_group,\n",
    "                    subscription_id=subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: cdh-azml-dev-mlw\n",
      "Azure region: eastus\n",
      "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
      "Resource group: CSELS-CDH-DEV\n"
     ]
    }
   ],
   "source": [
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\wsn8\\Code\\premier_analysis\\azure_ml\n",
      "\n",
      "Parent Directory: c:\\Users\\wsn8\\Code\\premier_analysis\n"
     ]
    }
   ],
   "source": [
    "# current working directory\n",
    "pwd = os.getcwd()\n",
    "print(\"Current Directory:\", pwd)\n",
    "  \n",
    "# parent directory\n",
    "parent = os.path.join(pwd, os.pardir)\n",
    "  \n",
    "# prints parent directory\n",
    "print(\"\\nParent Directory:\", os.path.abspath(parent))\n",
    "\n",
    "premier_path = os.path.abspath(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the existing cluster\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "clustername = 'StandardD13v2'\n",
    "is_new_cluster = False\n",
    "try:\n",
    "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
    "    print(\"Find the existing cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Cluster not find - Creating cluster.....\")\n",
    "    is_new_cluster = True\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
    "                                                           max_nodes=2)\n",
    "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
    "\n",
    "aml_compute_cpu.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/eastus/workspaces/d5539876-73f2-429b-9d16-cd4969e1602d/environments/premier_train_baseline_env/versions/4\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220915.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": true,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"premier_train_baseline_env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"default\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"matplotlib\",\n",
       "                        \"pandas\",\n",
       "                        \"argparse\",\n",
       "                        \"joblib\",\n",
       "                        \"scikit-learn\",\n",
       "                        \"azureml-sdk\",\n",
       "                        \"openpyxl\"\n",
       "                    ]\n",
       "                }\n",
       "            ]\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"4\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_train_baseline_env = Environment.from_conda_specification(name='premier_train_baseline_env', file_path='conda_dependencies_baseline.yml')\n",
    "# Specify a CPU base image\n",
    "premier_train_baseline_env.docker.enabled = True\n",
    "premier_train_baseline_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "premier_train_baseline_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Run\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "data_store = ws.get_default_datastore()\n",
    "\n",
    "##########Loading the data from datastore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./automl/baseline_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile ./automl/baseline_preprocessing.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "from azureml.core import Model,Dataset\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def baseline_preprocessing(OUTCOME):\n",
    "\n",
    "    DAY_ONE_ONLY = True\n",
    "    USE_DEMOG = True\n",
    "    TEST_SPLIT = 0.1\n",
    "    VAL_SPLIT = 0.2\n",
    "    RAND = 42\n",
    "\n",
    "    run = Run.get_context()\n",
    "    print(\"run name:\",run.display_name)\n",
    "    print(\"run details:\",run.get_details())\n",
    "    \n",
    "    ws = run.experiment.workspace\n",
    "\n",
    "    data_store = ws.get_default_datastore()\n",
    "\n",
    "    print(\"Creating dataset from Datastore\")\n",
    "    inputs = Dataset.File.from_files(path=data_store.path('output/pkl/trimmed_seqs.pkl'))  \n",
    "    vocab = Dataset.File.from_files(path=data_store.path('output/pkl/all_ftrs_dict.pkl'))\n",
    "    demog_dict = Dataset.File.from_files(path=data_store.path('output/pkl/demog_dict.pkl'))\n",
    "    cohort = Dataset.Tabular.from_delimited_files(path=data_store.path('output/cohort/cohort.csv'))\n",
    "    \n",
    "    pwd = os.path.dirname(__file__)\n",
    "    output_dir = os.path.abspath(os.path.join(pwd,\"output\"))\n",
    "    pkl_dir = os.path.join(output_dir, \"pkl\")\n",
    "    csv_dir = os.path.join(output_dir, \"csv\")\n",
    "\n",
    "    os.makedirs(pkl_dir, exist_ok=True)\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Dowloading data from Datastore...\")\n",
    "\n",
    "    inputs.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    vocab.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    demog_dict.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
    "    cohort.to_pandas_dataframe().to_csv(os.path.join(csv_dir,'cohort.csv'))\n",
    "\n",
    "    print(\"Loading var...\")\n",
    "    with open(os.path.join(pkl_dir, \"trimmed_seqs.pkl\"), \"rb\") as f:\n",
    "        inputs = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
    "        vocab = pkl.load(f)\n",
    "\n",
    "    with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
    "        demog_dict = pkl.load(f)\n",
    "        demog_dict = {k: v for v, k in demog_dict.items()}\n",
    "\n",
    "    \n",
    "    # Separating the inputs and labels\n",
    "    features = [t[0] for t in inputs]\n",
    "    demog = [t[1] for t in inputs]\n",
    "    cohort = pd.read_csv(os.path.join(csv_dir, 'cohort.csv'))\n",
    "    labels = cohort[OUTCOME]\n",
    "\n",
    "    # Counts to use for loops and stuff\n",
    "    n_patients = len(features)\n",
    "    n_features = np.max(list(vocab.keys()))\n",
    "    n_classes = len(np.unique(labels))\n",
    "    binary = n_classes <= 2\n",
    "\n",
    "        # Converting the labels to an array\n",
    "    y = np.array(labels, dtype=np.uint8)\n",
    "\n",
    "    # Optionally limiting the features to only those from the first day\n",
    "    # of the actual COVID visit\n",
    "    if DAY_ONE_ONLY:\n",
    "        features = [l[-1] for l in features]\n",
    "    else:\n",
    "        features = [flatten(l) for l in features]\n",
    "\n",
    "    new_demog = [[i + n_features for i in l] for l in demog]\n",
    "    features = [features[i] + new_demog[i] for i in range(n_patients)]\n",
    "    demog_vocab = {k + n_features: v for k, v in demog_dict.items()}\n",
    "    vocab.update(demog_vocab)\n",
    "    n_features = np.max([np.max(l) for l in features])\n",
    "    # all_feats.update({v: v for k, v in demog_dict.items()})\n",
    "\n",
    "    # Converting the features to a sparse matrix\n",
    "    mat = lil_matrix((n_patients, n_features + 1))\n",
    "    for row, cols in enumerate(features):\n",
    "        mat[row, cols] = 1\n",
    "\n",
    "    # Converting to csr because the internet said it would be faster\n",
    "    print(\"Converting to csr..\")\n",
    "    X = mat.tocsr()\n",
    "\n",
    "    # Splitting the data; 'all' will produce the same test sample\n",
    "    # for every outcome (kinda nice)\n",
    "\n",
    "    STRATIFY = None\n",
    "\n",
    "    strat_var = y\n",
    "    train, test = train_test_split(range(n_patients),\n",
    "                                    test_size=TEST_SPLIT,\n",
    "                                    stratify=strat_var,\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    # Doing a validation split for threshold-picking on binary problems\n",
    "    train, val = train_test_split(train,\n",
    "                                    test_size=VAL_SPLIT,\n",
    "                                    stratify=strat_var[train],\n",
    "                                    random_state=RAND)\n",
    "\n",
    "    return  X[train],y[train],X[test],y[test]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"feature\")\n",
    "    parser.add_argument(\"--outcome\",type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    OUTCOME = args.outcome\n",
    "\n",
    "    x_train,y_train,x_test,y_test = baseline_preprocessing(OUTCOME=OUTCOME)\n",
    "\n",
    "    x_train_df = pd.DataFrame.sparse.from_spmatrix(x_train)\n",
    "    x_test_df = pd.DataFrame.sparse.from_spmatrix(x_test)\n",
    "\n",
    "    train_data = pd.concat([x_train_df,pd.DataFrame(y_train)],axis =1)\n",
    "    print(\"train shape:\",train_data.shape)\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "\n",
    "    data_store = ws.get_default_datastore()\n",
    "\n",
    "    dataset_name = f\"automl-train-baseline-{OUTCOME}\"\n",
    "    ds_train = Dataset.Tabular.register_pandas_dataframe(train_data,target=data_store,name=dataset_name,show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_feature_processing created\n"
     ]
    }
   ],
   "source": [
    "OUTCOME = 'icu'\n",
    "source_directory ='./automl'\n",
    "job_feature_processing = ScriptRunConfig(\n",
    "                         script=\"baseline_preprocessing.py\", \n",
    "                         arguments=[\"--outcome\",OUTCOME],\n",
    "                         compute_target=aml_compute_cpu, \n",
    "                         environment=premier_train_baseline_env,\n",
    "                         source_directory=source_directory)\n",
    "print(\"job_feature_processing created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit Experiment: Job-feature-preprocess-automl-baseline\n"
     ]
    }
   ],
   "source": [
    "exp_name = f\"Job-feature-preprocess-automl-baseline\"\n",
    "print(\"Submit Experiment:\",exp_name)\n",
    "# Create experiment\n",
    "experiment = Experiment(workspace=ws, name = exp_name)\n",
    "run = experiment.submit(job_feature_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto ML configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "# Get the batch dataset for input\n",
    "dataset_name = f\"automl-train-baseline-{OUTCOME}\"\n",
    "batch_data_set = ws.datasets['premier_features']\n",
    "\n",
    "# Set parameters for AutoMLConfig\n",
    "# NOTE: DO NOT CHANGE THE experiment_timeout_minutes PARAMETER OR YOUR INSTANCE WILL TIME OUT.\n",
    "# If you wish to run the experiment longer, you will need to run this notebook in your own\n",
    "# Azure tenant, which will incur personal costs.\n",
    "automl_config = AutoMLConfig(\n",
    "    experiment_timeout_minutes=15,\n",
    "    task='classification',\n",
    "    primary_metric=\"accuracy\",\n",
    "    compute_target = aml_compute_cpu,\n",
    "    training_data=train_data,\n",
    "    label_column_name='y',\n",
    "    n_cross_validations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigException",
     "evalue": "ConfigException:\n\tMessage: Input of type '<class 'pandas.core.frame.DataFrame'>' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset]Please refer to documentation for converting to Supported types: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Input of type '<class 'pandas.core.frame.DataFrame'>' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset]Please refer to documentation for converting to Supported types: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py\",\n        \"details_uri\": \"https://aka.ms/AutoMLConfig\",\n        \"target\": \"training_data\",\n        \"inner_error\": {\n            \"code\": \"BadArgument\",\n            \"inner_error\": {\n                \"code\": \"ArgumentInvalid\",\n                \"inner_error\": {\n                    \"code\": \"InvalidInputDatatype\"\n                }\n            }\n        }\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Submit your automl run\u001b[39;00m\n\u001b[0;32m      2\u001b[0m exp \u001b[39m=\u001b[39m Experiment(workspace\u001b[39m=\u001b[39mws, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mautoml-train-baseline-icu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m automl_run \u001b[39m=\u001b[39m exp\u001b[39m.\u001b[39;49msubmit(automl_config, show_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\wsn8\\AppData\\Local\\conda\\conda\\envs\\premier_score_env\\lib\\site-packages\\azureml\\core\\experiment.py:238\u001b[0m, in \u001b[0;36mExperiment.submit\u001b[1;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m submit_func \u001b[39m=\u001b[39m get_experiment_submit(config)\n\u001b[0;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_context(\u001b[39m\"\u001b[39m\u001b[39msubmit config \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)):\n\u001b[1;32m--> 238\u001b[0m     run \u001b[39m=\u001b[39m submit_func(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkspace, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     run\u001b[39m.\u001b[39mset_tags(tags)\n",
      "File \u001b[1;32mc:\\Users\\wsn8\\AppData\\Local\\conda\\conda\\envs\\premier_score_env\\lib\\site-packages\\azureml\\train\\automl\\automlconfig.py:93\u001b[0m, in \u001b[0;36m_automl_static_submit\u001b[1;34m(automl_config_object, workspace, experiment_name, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m run_config \u001b[39m=\u001b[39m automl_config_object\u001b[39m.\u001b[39m_run_configuration\n\u001b[0;32m     91\u001b[0m compute_target \u001b[39m=\u001b[39m automl_config_object\u001b[39m.\u001b[39muser_settings\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcompute_target\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m automl_config_object\u001b[39m.\u001b[39;49m_validate_config_settings(workspace)\n\u001b[0;32m     94\u001b[0m fit_params \u001b[39m=\u001b[39m automl_config_object\u001b[39m.\u001b[39m_get_fit_params()\n\u001b[0;32m     96\u001b[0m \u001b[39m# retrieve settings which are present in user but not part of fit_params\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wsn8\\AppData\\Local\\conda\\conda\\envs\\premier_score_env\\lib\\site-packages\\azureml\\train\\automl\\automlconfig.py:2169\u001b[0m, in \u001b[0;36mAutoMLConfig._validate_config_settings\u001b[1;34m(self, workspace)\u001b[0m\n\u001b[0;32m   2166\u001b[0m         input_type \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_settings\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m   2167\u001b[0m         target \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 2169\u001b[0m     \u001b[39mraise\u001b[39;00m ConfigException\u001b[39m.\u001b[39m_with_error(\n\u001b[0;32m   2170\u001b[0m         AzureMLError\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m   2171\u001b[0m             InvalidInputDatatype, target\u001b[39m=\u001b[39mtarget, input_type\u001b[39m=\u001b[39minput_type,\n\u001b[0;32m   2172\u001b[0m             supported_types\u001b[39m=\u001b[39mSupportedInputDatatypes\u001b[39m.\u001b[39mTABULAR_DATASET\n\u001b[0;32m   2173\u001b[0m         )\n\u001b[0;32m   2174\u001b[0m     )\n\u001b[0;32m   2175\u001b[0m \u001b[39mif\u001b[39;00m workspace \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m compute_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2176\u001b[0m     all_compute_targets \u001b[39m=\u001b[39m workspace\u001b[39m.\u001b[39mcompute_targets\n",
      "\u001b[1;31mConfigException\u001b[0m: ConfigException:\n\tMessage: Input of type '<class 'pandas.core.frame.DataFrame'>' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset]Please refer to documentation for converting to Supported types: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Input of type '<class 'pandas.core.frame.DataFrame'>' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset]Please refer to documentation for converting to Supported types: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py\",\n        \"details_uri\": \"https://aka.ms/AutoMLConfig\",\n        \"target\": \"training_data\",\n        \"inner_error\": {\n            \"code\": \"BadArgument\",\n            \"inner_error\": {\n                \"code\": \"ArgumentInvalid\",\n                \"inner_error\": {\n                    \"code\": \"InvalidInputDatatype\"\n                }\n            }\n        }\n    }\n}"
     ]
    }
   ],
   "source": [
    "# Submit your automl run\n",
    "exp = Experiment(workspace=ws, name=\"automl-train-baseline-icu\")\n",
    "automl_run = exp.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('premier_score_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc78c87736aeb3cc28398dbcb4d22314bd77bb9dc1f4148cd78e25835521c0d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
