{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for  Premier Analysis on Azure Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sklearn\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
    "from azureml.core import Run, Dataset, Environment,Experiment,ScriptRunConfig\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import  Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: cdh-azml-dev-mlw\n",
      "Azure region: eastus\n",
      "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
      "Resource group: CSELS-CDH-DEV\n"
     ]
    }
   ],
   "source": [
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\wsn8\\Code\\premier_analysis\\azure_ml\n",
      "\n",
      "Parent Directory: c:\\Users\\wsn8\\Code\\premier_analysis\n"
     ]
    }
   ],
   "source": [
    "# current working directory\n",
    "path = os.getcwd()\n",
    "print(\"Current Directory:\", path)\n",
    "  \n",
    "# parent directory\n",
    "parent = os.path.join(path, os.pardir)\n",
    "  \n",
    "# prints parent directory\n",
    "print(\"\\nParent Directory:\", os.path.abspath(parent))\n",
    "\n",
    "premier_path = os.path.abspath(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the existing cluster\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "clustername = 'StandardD13v2'\n",
    "is_new_cluster = False\n",
    "try:\n",
    "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
    "    print(\"Find the existing cluster\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Cluster not find - Creating cluster.....\")\n",
    "    is_new_cluster = True\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
    "                                                           max_nodes=2)\n",
    "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
    "\n",
    "aml_compute_cpu.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210220.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"premier_feature_env\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"default\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"matplotlib\",\n",
       "                        \"pandas==1.1.5\",\n",
       "                        \"argparse\",\n",
       "                        \"joblib\",\n",
       "                        \"scikit-learn\",\n",
       "                        \"azureml-sdk\",\n",
       "                        \"openpyxl\"\n",
       "                    ]\n",
       "                }\n",
       "            ]\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"6\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premier_feature_env = Environment.from_conda_specification(name='premier_feature_env', file_path='./environments/conda_dependencies_features.yml')\n",
    "# Specify a CPU base image\n",
    "#premier_feature_env.docker.enabled = True\n",
    "premier_feature_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "premier_feature_env.register(workspace=ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore's name: edav_dev_ds\n",
      "DataReference object created\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "\n",
    "datastore_name = 'edav_dev_ds'\n",
    "cdh_path = 'exploratory/databricks_ml/mitre_premier/data/'\n",
    "ds = Datastore.get(ws, datastore_name)\n",
    "\n",
    "print(\"Datastore's name: {}\".format(ds.name))\n",
    "\n",
    "premier_data_ref = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name='premier_data',\n",
    "    path_on_datastore=cdh_path)\n",
    "print(\"DataReference object created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_store = ws.get_default_datastore()\n",
    "flat_features = OutputFileDatasetConfig(destination=(data_store, 'output/parquet'))\n",
    "feature_lookup = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory ='./training'\n",
    "job_feature_extraction = ScriptRunConfig(\n",
    "                         script=\"feature_extraction.py\", \n",
    "                         arguments=[\"--flat_features\",flat_features,\"--feature_lookup\",feature_lookup],\n",
    "                         compute_target=aml_compute_cpu, \n",
    "                         environment=premier_feature_env,\n",
    "                         source_directory=source_directory)\n",
    "           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit Experiment\n"
     ]
    }
   ],
   "source": [
    "# Create experiment\n",
    "experiment_extraction = Experiment(workspace=ws, name=f\"Job-feature-extraction\")\n",
    "    \n",
    "print(\"Submit Experiment\")\n",
    "run_extraction = experiment_extraction.submit(job_feature_extraction)\n",
    "run_extraction.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_seq = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))\n",
    "pat_data = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))\n",
    "demog_dict = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))\n",
    "all_ftrs_dict = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))\n",
    "int_seqs = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory ='./training'\n",
    "job_feature_tokenization = ScriptRunConfig(script=\"feature_tokenization.py\", \n",
    "                         arguments=[\"--flat_features\",flat_features,\n",
    "                                    \"--trimmed_seq_file\",trimmed_seq,\n",
    "                                    \"--pat_data_file\",pat_data,\n",
    "                                    \"--demog_dict_file\",demog_dict,\n",
    "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
    "                                    \"--int_seqs_file\",int_seqs],\n",
    "                         compute_target=aml_compute_cpu, \n",
    "                         environment=premier_feature_env,\n",
    "                         source_directory=source_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit Experiment\n"
     ]
    }
   ],
   "source": [
    "# Create experiment\n",
    "experiment_tokenization = Experiment(workspace=ws, name=f\"Job-feature-tokenization\")\n",
    "    \n",
    "print(\"Submit Experiment\")\n",
    "run_tokenization = experiment_tokenization.submit(job_feature_tokenization)\n",
    "run_tokenization.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_seq_pkl = OutputFileDatasetConfig(destination=(data_store, 'output/pkl'))\n",
    "cohort = OutputFileDatasetConfig(destination=(data_store, 'output/cohort'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sequence_trimming created\n"
     ]
    }
   ],
   "source": [
    "source_directory ='./training'\n",
    "job_sequence_trimming = ScriptRunConfig(\n",
    "                         script=\"sequence_trimming.py\", \n",
    "                         arguments=[\"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\n",
    "                                    \"--pat_data_file\",pat_data,\n",
    "                                    \"--feature_lookup\",feature_lookup,\n",
    "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
    "                                    \"--int_seqs_file\",int_seqs,\n",
    "                                    \"--cohort\",cohort],\n",
    "                         compute_target=aml_compute_cpu, \n",
    "                         environment=premier_feature_env, \n",
    "                         source_directory=source_directory)\n",
    "print(\" sequence_trimming created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit Experiment\n"
     ]
    }
   ],
   "source": [
    "# Create experiment\n",
    "experiment_trimming = Experiment(workspace=ws, name=f\"Job-feature-sequence-trimming\")\n",
    "    \n",
    "print(\"Submit Experiment\")\n",
    "run_trimming = experiment_trimming.submit(job_sequence_trimming)\n",
    "run_trimming.wait_for_completion(show_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('azureml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c29d249f6248e1876db3a43ab8bde2e53ec2cf908dd5eb31493a2d1f2322e9b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
