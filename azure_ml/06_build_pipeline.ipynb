{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build Deep Learning Pipeline - Premier Analysis on Azure Machine Learning\r\n",
        "### LSTM and DAN "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\r\n",
        "import azureml\r\n",
        "import os\r\n",
        "import sklearn\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import f1_score,accuracy_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from azureml.core import Workspace, Experiment, Run, RunConfiguration\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "from azureml.core import ScriptRunConfig, Environment\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, DEFAULT_GPU_IMAGE\r\n",
        "\r\n",
        "# check core SDK version number\r\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\r\n",
        "print(sklearn.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Azure ML SDK Version:  1.43.0\n1.1.2\n"
        }
      ],
      "execution_count": 167,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897201221
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import  Workspace\r\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"9ce70869-60db-44fd-abe8-d2767077fc8f\")\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Workspace name: ' + ws.name, \r\n",
        "      'Azure region: ' + ws.location, \r\n",
        "      'Subscription id: ' + ws.subscription_id, \r\n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: cdh-azml-dev-mlw\nAzure region: eastus\nSubscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\nResource group: csels-cdh-dev\n"
        }
      ],
      "execution_count": 168,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897324034
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# current working directory\r\n",
        "path = os.getcwd()\r\n",
        "print(\"Current Directory:\", path)\r\n",
        "  \r\n",
        "# parent directory\r\n",
        "parent = os.path.join(path, os.pardir)\r\n",
        "  \r\n",
        "# prints parent directory\r\n",
        "print(\"\\nParent Directory:\", os.path.abspath(parent))\r\n",
        "\r\n",
        "premier_path = os.path.abspath(parent)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Current Directory: /mnt/batch/tasks/shared/LS_root/mounts/clusters/wsn8-su2/code/Users/WSN8-SU/premier_analysis/azure_ml\n\nParent Directory: /mnt/batch/tasks/shared/LS_root/mounts/clusters/wsn8-su2/code/Users/WSN8-SU/premier_analysis\n"
        }
      ],
      "execution_count": 169,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897324199
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create CPU Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustername = 'StandardD13v2'\r\n",
        "is_new_cluster = False\r\n",
        "try:\r\n",
        "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\r\n",
        "    print(\"Find the existing cluster\")\r\n",
        "except ComputeTargetException:\r\n",
        "    print(\"Cluster not find - Creating cluster.....\")\r\n",
        "    is_new_cluster = True\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\r\n",
        "                                                           max_nodes=2)\r\n",
        "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\r\n",
        "\r\n",
        "aml_compute_cpu.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Find the existing cluster\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 170,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897363972
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create GPU Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustername = 'StandardNC6'\r\n",
        "is_new_cluster = False\r\n",
        "try:\r\n",
        "    aml_cluster_gpu = ComputeTarget(workspace = ws,name= clustername)\r\n",
        "    print(\"Find the existing cluster\")\r\n",
        "except ComputeTargetException:\r\n",
        "    print(\"Cluster not find - Creating cluster.....\")\r\n",
        "    is_new_cluster = True\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='StandardNC6',\r\n",
        "                                                           max_nodes=2)\r\n",
        "    aml_cluster_gpu = ComputeTarget.create(ws, clustername, compute_config)\r\n",
        "\r\n",
        "aml_cluster_gpu.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Find the existing cluster\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 171,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897405078
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies_features.yml\r\n",
        "\r\n",
        "channels:\r\n",
        "- anaconda\r\n",
        "- default\r\n",
        "dependencies:\r\n",
        "- python=3.8\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - matplotlib\r\n",
        "  - pandas==1.1.5\r\n",
        "  - argparse\r\n",
        "  - joblib\r\n",
        "  - scikit-learn\r\n",
        "  - azureml-sdk\r\n",
        "  - openpyxl"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda_dependencies_features.yml\n"
        }
      ],
      "execution_count": 172,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies_model.yml\r\n",
        "channels:\r\n",
        "- anaconda\r\n",
        "- default\r\n",
        "dependencies:\r\n",
        "- python=3.8\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - matplotlib\r\n",
        "  - pandas\r\n",
        "  - argparse\r\n",
        "  - joblib\r\n",
        "  - scikit-learn\r\n",
        "  - azureml-sdk\r\n",
        "  - openpyxl\r\n",
        "  - tensorflow\r\n",
        "  - keras-tuner"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting conda_dependencies_model.yml\n"
        }
      ],
      "execution_count": 173,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "premier_feature_env = Environment.from_conda_specification(name='premier_feature_env', file_path='conda_dependencies_features.yml')\r\n",
        "# Specify a CPU base image\r\n",
        "#premier_feature_env.docker.enabled = True\r\n",
        "premier_feature_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "premier_feature_env.register(workspace=ws)\r\n",
        "run_config_feature = RunConfiguration()\r\n",
        "run_config_feature.environment = premier_feature_env"
      ],
      "outputs": [],
      "execution_count": 174,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897408914
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "premier_train_model_env = Environment.from_conda_specification(name='premier_train_model_env', file_path='conda_dependencies_model.yml')\r\n",
        "# Specify a GPU base image\r\n",
        "# premier_train_model_env.docker.enabled = True\r\n",
        "premier_train_model_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "premier_train_model_env.register(workspace=ws)\r\n",
        "run_config_train = RunConfiguration()\r\n",
        "run_config_train.environment = premier_train_model_env"
      ],
      "outputs": [],
      "execution_count": 175,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897411909
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.datastore import Datastore\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "\r\n",
        "\r\n",
        "datastore_name = 'edav_dev_ds'\r\n",
        "cdh_path = 'exploratory/databricks_ml/mitre_premier/data/'\r\n",
        "ds = Datastore.get(ws, datastore_name)\r\n",
        "\r\n",
        "print(\"Datastore's name: {}\".format(ds.name))\r\n",
        "\r\n",
        "premier_data_ref = DataReference(\r\n",
        "    datastore=ds,\r\n",
        "    data_reference_name='premier_data',\r\n",
        "    path_on_datastore=cdh_path)\r\n",
        "print(\"DataReference object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datastore's name: edav_dev_ds\nDataReference object created\n"
        }
      ],
      "execution_count": 176,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664897414923
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_store = ws.get_default_datastore()\r\n",
        "flat_features = PipelineData(\"flat_features_data\",datastore=data_store).as_dataset()\r\n",
        "feature_lookup = PipelineData(\"feature_lookup_data\",datastore=data_store).as_dataset()\r\n",
        "trimmed_seq = PipelineData(\"trimmed_seq\",datastore=data_store).as_dataset()\r\n",
        "pat_data = PipelineData(\"pat_data\",datastore=data_store).as_dataset()\r\n",
        "demog_dict = PipelineData(\"demog_dict_data\",datastore=data_store).as_dataset()\r\n",
        "all_ftrs_dict = PipelineData(\"all_ftrs_dict_data\",datastore=data_store).as_dataset()\r\n",
        "int_seqs = PipelineData(\"int_seqs_data\",datastore=data_store).as_dataset()\r\n",
        "\r\n",
        "trimmed_seq_pkl = PipelineData(\"trimmed_seq_pkl_data\",datastore=data_store).as_dataset()\r\n",
        "cohort = PipelineData(\"cohort\",datastore=data_store).as_dataset()\r\n",
        "model_file = PipelineData(\"model_probs\",datastore=data_store).as_dataset()\r\n",
        "stats_file = PipelineData(\"analysis_data\",datastore=data_store).as_dataset()\r\n",
        "preds_file = PipelineData(\"prediction_data\",datastore=data_store).as_dataset()\r\n",
        "probs_file = PipelineData(\"probs_data\",datastore=data_store).as_dataset()"
      ],
      "outputs": [],
      "execution_count": 188,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664901029904
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./training'\r\n",
        "step1 = PythonScriptStep(name=\"feature_extraction\",\r\n",
        "                         script_name=\"feature_extraction.py\", \r\n",
        "                         inputs=[premier_data_ref.as_download()],\r\n",
        "                         arguments=[\"--flat_features\",flat_features,\"--feature_lookup\",feature_lookup],\r\n",
        "                         outputs=[flat_features,feature_lookup],\r\n",
        "                         compute_target=aml_compute_cpu, \r\n",
        "                         runconfig=run_config_feature,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step1 feature_extraction created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step1 feature_extraction created\n"
        }
      ],
      "execution_count": 189,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664901031973
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Tokenization"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./training'\r\n",
        "step2 = PythonScriptStep(name=\"feature_tokenization\",\r\n",
        "                         script_name=\"feature_tokenization.py\", \r\n",
        "                         inputs=[flat_features],\r\n",
        "                         arguments=[\"--flat_features\",flat_features,\r\n",
        "                                    \"--trimmed_seq_file\",trimmed_seq,\r\n",
        "                                    \"--pat_data_file\",pat_data,\r\n",
        "                                    \"--demog_dict_file\",demog_dict,\r\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\r\n",
        "                                    \"--int_seqs_file\",int_seqs],\r\n",
        "                         outputs=[trimmed_seq,demog_dict,pat_data,all_ftrs_dict,int_seqs],\r\n",
        "                         compute_target=aml_compute_cpu, \r\n",
        "                         runconfig=run_config_feature,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step2 feature_tokenization created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step2 feature_tokenization created\n"
        }
      ],
      "execution_count": 190,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664901032956
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Trimming"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./training'\r\n",
        "step3 = PythonScriptStep(name=\"sequence_trimming\",\r\n",
        "                         script_name=\"sequence_trimming.py\", \r\n",
        "                         inputs=[pat_data,all_ftrs_dict,int_seqs,feature_lookup],\r\n",
        "                         arguments=[\"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\r\n",
        "                                    \"--pat_data_file\",pat_data,\r\n",
        "                                    \"--feature_lookup\",feature_lookup,\r\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\r\n",
        "                                    \"--int_seqs_file\",int_seqs,\r\n",
        "                                    \"--cohort\",cohort],\r\n",
        "                         outputs=[trimmed_seq_pkl,cohort],\r\n",
        "                         compute_target=aml_compute_cpu, \r\n",
        "                         runconfig=run_config_feature,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step3 sequence_trimming created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step3 sequence_trimming created\n"
        }
      ],
      "execution_count": 191,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664901035015
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Step"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./training'\r\n",
        "step4= PythonScriptStep(name=\"train_model\",\r\n",
        "                         script_name=\"train_model_step.py\", \r\n",
        "                         inputs=[demog_dict,trimmed_seq_pkl,all_ftrs_dict,feature_lookup,cohort],\r\n",
        "                         arguments=[\"--model_file\",model_file,\r\n",
        "                                    \"--stats_file\",stats_file,\r\n",
        "                                    \"--preds_file\",preds_file, \r\n",
        "                                    \"--probs_file\",probs_file,   \r\n",
        "                                    \"--demog_dict_file\",demog_dict,\r\n",
        "                                    \"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\r\n",
        "                                    \"--feature_lookup\",feature_lookup,\r\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\r\n",
        "                                    \"--cohort\",cohort],\r\n",
        "                         outputs=[model_file,stats_file,preds_file,probs_file],\r\n",
        "                         compute_target=aml_cluster_gpu, \r\n",
        "                         runconfig=run_config_train,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step4 train_model created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step4 train_model created\n"
        }
      ],
      "execution_count": 192,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664901097034
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory ='./training/register'\r\n",
        "step5= PythonScriptStep(name=\"register_model\",\r\n",
        "                         script_name=\"register_model.py\", \r\n",
        "                         inputs=[model_file],\r\n",
        "                         arguments=[\"--model_file\",model_file],\r\n",
        "                         compute_target=aml_cluster_gpu, \r\n",
        "                         runconfig=run_config_train,\r\n",
        "                         source_directory=source_directory,\r\n",
        "                         allow_reuse=True)\r\n",
        "print(\"Step5 register_model created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step5 register_model created\n"
        }
      ],
      "execution_count": 197,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664904048017
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [step1,step2,step3,step4,step5]\r\n",
        "pipeline1 = Pipeline(workspace=ws,steps=steps,default_datastore=data_store)"
      ],
      "outputs": [],
      "execution_count": 198,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664904052917
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_exp = Experiment(workspace=ws, name=\"Premier-Feature-Pipeline-3\")"
      ],
      "outputs": [],
      "execution_count": 199,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664904055957
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_exp.submit(pipeline1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step feature_extraction [ab154d49][86dc60af-ff47-4d8a-9a35-f3a117095b5c], (This step will run and generate new outputs)\nCreated step feature_tokenization [453b55d5][a3db32d6-8886-4ebb-9e1b-09ad9579aa28], (This step will run and generate new outputs)Created step sequence_trimming [fa18507f][80dd786d-ed24-4592-ae74-82396a13638b], (This step will run and generate new outputs)\n\nCreated step train_model [c67c4afd][2fb4f67d-3753-4787-a342-7208cf79e488], (This step will run and generate new outputs)\nCreated step register_model [7e735cd1][a3c6a8e0-aa63-43f1-bc3a-9c546d96f09e], (This step will run and generate new outputs)\nUsing data reference premier_data for StepId [9bc0a31f][b2d198f2-4469-4899-9ebe-e6a54440d08a], (Consumers of this data are eligible to reuse prior runs.)\nSubmitted PipelineRun 7ced846d-95c3-4569-8e03-244aeca3f226\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/7ced846d-95c3-4569-8e03-244aeca3f226?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&tid=9ce70869-60db-44fd-abe8-d2767077fc8f\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 200,
          "data": {
            "text/plain": "Run(Experiment: Premier-Feature-Pipeline-3,\nId: 7ced846d-95c3-4569-8e03-244aeca3f226,\nType: azureml.PipelineRun,\nStatus: Running)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Premier-Feature-Pipeline-3</td><td>7ced846d-95c3-4569-8e03-244aeca3f226</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/runs/7ced846d-95c3-4569-8e03-244aeca3f226?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&amp;tid=9ce70869-60db-44fd-abe8-d2767077fc8f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 200,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1664904126089
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}