{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Build Deep Learning Pipeline - Premier Analysis on Azure Machine Learning\n",
        "### LSTM and DAN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1664897201221
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.24.0\n",
            "1.0.2\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import azureml\n",
        "import os\n",
        "import sklearn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from azureml.core import Run, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import ScriptRunConfig, Environment\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, DEFAULT_GPU_IMAGE\n",
        "\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1664897324034
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: cdh-azml-dev-mlw\n",
            "Azure region: eastus\n",
            "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
            "Resource group: CSELS-CDH-DEV\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import  Workspace\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"9ce70869-60db-44fd-abe8-d2767077fc8f\")\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1664897324199
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Directory: c:\\Users\\wsn8\\Code\\premier_analysis\\azure_ml\n",
            "\n",
            "Parent Directory: c:\\Users\\wsn8\\Code\\premier_analysis\n"
          ]
        }
      ],
      "source": [
        "# current working directory\n",
        "path = os.getcwd()\n",
        "print(\"Current Directory:\", path)\n",
        "  \n",
        "# parent directory\n",
        "parent = os.path.join(path, os.pardir)\n",
        "  \n",
        "# prints parent directory\n",
        "print(\"\\nParent Directory:\", os.path.abspath(parent))\n",
        "\n",
        "premier_path = os.path.abspath(parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create CPU Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1664897363972
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardD13v2'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
        "                                                           max_nodes=2)\n",
        "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_compute_cpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create GPU Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1664897405078
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardNC6'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_cluster_gpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='StandardNC6',\n",
        "                                                           max_nodes=2)\n",
        "    aml_cluster_gpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_cluster_gpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting conda_dependencies_features.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile conda_dependencies_features.yml\n",
        "\n",
        "channels:\n",
        "- anaconda\n",
        "- default\n",
        "dependencies:\n",
        "- python=3.8\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - matplotlib\n",
        "  - pandas==1.1.5\n",
        "  - argparse\n",
        "  - joblib\n",
        "  - scikit-learn\n",
        "  - azureml-sdk\n",
        "  - openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting conda_dependencies_model.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile conda_dependencies_model.yml\n",
        "channels:\n",
        "- anaconda\n",
        "- default\n",
        "dependencies:\n",
        "- python=3.8\n",
        "- pip:\n",
        "  - azureml-defaults\n",
        "  - matplotlib\n",
        "  - pandas\n",
        "  - argparse\n",
        "  - joblib\n",
        "  - scikit-learn\n",
        "  - azureml-sdk\n",
        "  - openpyxl\n",
        "  - tensorflow\n",
        "  - keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1664897408914
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "premier_feature_env = Environment.from_conda_specification(name='premier_feature_env', file_path='conda_dependencies_features.yml')\n",
        "# Specify a CPU base image\n",
        "#premier_feature_env.docker.enabled = True\n",
        "premier_feature_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "premier_feature_env.register(workspace=ws)\n",
        "run_config_feature = RunConfiguration()\n",
        "run_config_feature.environment = premier_feature_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1664897411909
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "premier_train_model_env = Environment.from_conda_specification(name='premier_train_model_env', file_path='conda_dependencies_model.yml')\n",
        "# Specify a GPU base image\n",
        "# premier_train_model_env.docker.enabled = True\n",
        "premier_train_model_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "premier_train_model_env.register(workspace=ws)\n",
        "run_config_train = RunConfiguration()\n",
        "run_config_train.environment = premier_train_model_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "gather": {
          "logged": 1664897414923
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datastore's name: edav_dev_ds\n",
            "DataReference object created\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "\n",
        "datastore_name = 'edav_dev_ds'\n",
        "cdh_path = 'exploratory/databricks_ml/mitre_premier/data/'\n",
        "ds = Datastore.get(ws, datastore_name)\n",
        "\n",
        "print(\"Datastore's name: {}\".format(ds.name))\n",
        "\n",
        "premier_data_ref = DataReference(\n",
        "    datastore=ds,\n",
        "    data_reference_name='premier_data',\n",
        "    path_on_datastore=cdh_path)\n",
        "print(\"DataReference object created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "gather": {
          "logged": 1664901029904
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "data_store = ws.get_default_datastore()\n",
        "flat_features = PipelineData(\"flat_features_data\",datastore=data_store).as_dataset()\n",
        "feature_lookup = PipelineData(\"feature_lookup_data\",datastore=data_store).as_dataset()\n",
        "trimmed_seq = PipelineData(\"trimmed_seq\",datastore=data_store).as_dataset()\n",
        "pat_data = PipelineData(\"pat_data\",datastore=data_store).as_dataset()\n",
        "demog_dict = PipelineData(\"demog_dict_data\",datastore=data_store).as_dataset()\n",
        "all_ftrs_dict = PipelineData(\"all_ftrs_dict_data\",datastore=data_store).as_dataset()\n",
        "int_seqs = PipelineData(\"int_seqs_data\",datastore=data_store).as_dataset()\n",
        "\n",
        "trimmed_seq_pkl = PipelineData(\"trimmed_seq_pkl_data\",datastore=data_store).as_dataset()\n",
        "cohort = PipelineData(\"cohort\",datastore=data_store).as_dataset()\n",
        "model_file = PipelineData(\"model_probs\",datastore=data_store).as_dataset()\n",
        "stats_file = PipelineData(\"analysis_data\",datastore=data_store).as_dataset()\n",
        "preds_file = PipelineData(\"prediction_data\",datastore=data_store).as_dataset()\n",
        "probs_file = PipelineData(\"probs_data\",datastore=data_store).as_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = PipelineParameter(name=\"model_name\", default_value=\"dan\")\n",
        "outcome = PipelineParameter(name=\"outcome\", default_value=\"misa_pt\")\n",
        "n_epochs = PipelineParameter(name=\"n_epochs\",default_value=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "gather": {
          "logged": 1664901031973
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step1 feature_extraction created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step1 = PythonScriptStep(name=\"feature_extraction\",\n",
        "                         script_name=\"feature_extraction.py\", \n",
        "                         inputs=[premier_data_ref.as_download()],\n",
        "                         arguments=[\"--flat_features\",flat_features,\"--feature_lookup\",feature_lookup],\n",
        "                         outputs=[flat_features,feature_lookup],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step1 feature_extraction created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Feature Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "gather": {
          "logged": 1664901032956
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step2 feature_tokenization created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step2 = PythonScriptStep(name=\"feature_tokenization\",\n",
        "                         script_name=\"feature_tokenization.py\", \n",
        "                         inputs=[flat_features],\n",
        "                         arguments=[\"--flat_features\",flat_features,\n",
        "                                    \"--trimmed_seq_file\",trimmed_seq,\n",
        "                                    \"--pat_data_file\",pat_data,\n",
        "                                    \"--demog_dict_file\",demog_dict,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--int_seqs_file\",int_seqs],\n",
        "                         outputs=[trimmed_seq,demog_dict,pat_data,all_ftrs_dict,int_seqs],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step2 feature_tokenization created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Sequence Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "gather": {
          "logged": 1664901035015
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step3 sequence_trimming created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step3 = PythonScriptStep(name=\"sequence_trimming\",\n",
        "                         script_name=\"sequence_trimming.py\", \n",
        "                         inputs=[pat_data,all_ftrs_dict,int_seqs,feature_lookup],\n",
        "                         arguments=[\"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\n",
        "                                    \"--pat_data_file\",pat_data,\n",
        "                                    \"--feature_lookup\",feature_lookup,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--int_seqs_file\",int_seqs,\n",
        "                                    \"--cohort\",cohort],\n",
        "                         outputs=[trimmed_seq_pkl,cohort],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step3 sequence_trimming created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "gather": {
          "logged": 1664901097034
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step4 train_model created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step4= PythonScriptStep(name=\"train_model\",\n",
        "                         script_name=\"train_model_step.py\", \n",
        "                         inputs=[demog_dict,trimmed_seq_pkl,all_ftrs_dict,feature_lookup,cohort],\n",
        "                         arguments=[\"--model\",model_name,\n",
        "                                    \"--outcome\",outcome,\n",
        "                                    \"--epochs\",n_epochs,\n",
        "                                    \"--model_file\",model_file,\n",
        "                                    \"--stats_file\",stats_file,\n",
        "                                    \"--preds_file\",preds_file, \n",
        "                                    \"--probs_file\",probs_file,   \n",
        "                                    \"--demog_dict_file\",demog_dict,\n",
        "                                    \"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\n",
        "                                    \"--feature_lookup\",feature_lookup,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--cohort\",cohort],\n",
        "                         outputs=[model_file,stats_file,preds_file,probs_file],\n",
        "                         compute_target=aml_cluster_gpu, \n",
        "                         runconfig=run_config_train,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step4 train_model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Register model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "gather": {
          "logged": 1664904048017
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step5 register_model created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training/register'\n",
        "step5= PythonScriptStep(name=\"register_model\",\n",
        "                         script_name=\"register_model.py\", \n",
        "                         inputs=[model_file],\n",
        "                         arguments=[\"--model_file\",model_file,\n",
        "                                     \"--model\",model_name,\n",
        "                                     \"--outcome\",outcome],\n",
        "                         compute_target=aml_cluster_gpu, \n",
        "                         runconfig=run_config_train,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step5 register_model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "gather": {
          "logged": 1664904052917
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "steps = [step1,step2,step3,step4,step5]\n",
        "pipeline1 = Pipeline(workspace=ws,steps=steps,default_datastore=data_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "gather": {
          "logged": 1664904055957
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "run_exp = Experiment(workspace=ws, name=\"Premier-Feature-Pipeline-3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "gather": {
          "logged": 1664904126089
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step feature_extraction [ab154d49][86dc60af-ff47-4d8a-9a35-f3a117095b5c], (This step will run and generate new outputs)\n",
            "Created step feature_tokenization [453b55d5][a3db32d6-8886-4ebb-9e1b-09ad9579aa28], (This step will run and generate new outputs)Created step sequence_trimming [fa18507f][80dd786d-ed24-4592-ae74-82396a13638b], (This step will run and generate new outputs)\n",
            "\n",
            "Created step train_model [c67c4afd][2fb4f67d-3753-4787-a342-7208cf79e488], (This step will run and generate new outputs)\n",
            "Created step register_model [7e735cd1][a3c6a8e0-aa63-43f1-bc3a-9c546d96f09e], (This step will run and generate new outputs)\n",
            "Using data reference premier_data for StepId [9bc0a31f][b2d198f2-4469-4899-9ebe-e6a54440d08a], (Consumers of this data are eligible to reuse prior runs.)\n",
            "Submitted PipelineRun 7ced846d-95c3-4569-8e03-244aeca3f226\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7ced846d-95c3-4569-8e03-244aeca3f226?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&tid=9ce70869-60db-44fd-abe8-d2767077fc8f\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Premier-Feature-Pipeline-3</td><td>7ced846d-95c3-4569-8e03-244aeca3f226</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/runs/7ced846d-95c3-4569-8e03-244aeca3f226?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/csels-cdh-dev/workspaces/cdh-azml-dev-mlw&amp;tid=9ce70869-60db-44fd-abe8-d2767077fc8f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: Premier-Feature-Pipeline-3,\n",
              "Id: 7ced846d-95c3-4569-8e03-244aeca3f226,\n",
              "Type: azureml.PipelineRun,\n",
              "Status: Running)"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_exp.submit(pipeline1,pipeline_parameters={\"model_name\": \"lstm\",\n",
        "                                            \"outcome\":'icu',\n",
        "                                            \"n_epochs\":1})"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('azureml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c29d249f6248e1876db3a43ab8bde2e53ec2cf908dd5eb31493a2d1f2322e9b6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
