{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Build Deep Learning Pipeline - Premier Analysis on Azure Machine Learning\n",
        "### LSTM and DAN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1664897201221
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure ML SDK Version:  1.24.0\n",
            "1.0.2\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import azureml\n",
        "import os\n",
        "import sklearn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from azureml.core import Run, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import ScriptRunConfig, Environment\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, DEFAULT_GPU_IMAGE\n",
        "\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1664897324034
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: cdh-azml-dev-mlw\n",
            "Azure region: eastus\n",
            "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
            "Resource group: CSELS-CDH-DEV\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import  Workspace\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1664897324199
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Directory: c:\\Users\\wsn8\\Code\\premier_analysis\\azure_ml\n",
            "\n",
            "Parent Directory: c:\\Users\\wsn8\\Code\\premier_analysis\n"
          ]
        }
      ],
      "source": [
        "# current working directory\n",
        "path = os.getcwd()\n",
        "print(\"Current Directory:\", path)\n",
        "  \n",
        "# parent directory\n",
        "parent = os.path.join(path, os.pardir)\n",
        "  \n",
        "# prints parent directory\n",
        "print(\"\\nParent Directory:\", os.path.abspath(parent))\n",
        "\n",
        "premier_path = os.path.abspath(parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create CPU Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1664897363972
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardD13v2'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
        "                                                           max_nodes=2)\n",
        "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_compute_cpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create GPU Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1664897405078
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardNC6'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_cluster_gpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='StandardNC6',\n",
        "                                                           max_nodes=2)\n",
        "    aml_cluster_gpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_cluster_gpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1664897408914
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "premier_feature_env = Environment.from_conda_specification(name='premier_feature_env', file_path='./environments/conda_dependencies_features.yml')\n",
        "# Specify a CPU base image\n",
        "#premier_feature_env.docker.enabled = True\n",
        "premier_feature_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "premier_feature_env.register(workspace=ws)\n",
        "run_config_feature = RunConfiguration()\n",
        "run_config_feature.environment = premier_feature_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1664897411909
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "premier_train_model_env = Environment.from_conda_specification(name='premier_train_model_env', file_path='./environments/conda_dependencies_model.yml')\n",
        "# Specify a GPU base image\n",
        "# premier_train_model_env.docker.enabled = True\n",
        "premier_train_model_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "premier_train_model_env.register(workspace=ws)\n",
        "run_config_train = RunConfiguration()\n",
        "run_config_train.environment = premier_train_model_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data source reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1664897414923
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datastore's name: edav_dev_ds\n",
            "DataReference object created\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "\n",
        "datastore_name = 'edav_dev_ds'\n",
        "cdh_path = 'exploratory/databricks_ml/mitre_premier/data/'\n",
        "ds = Datastore.get(ws, datastore_name)\n",
        "\n",
        "print(\"Datastore's name: {}\".format(ds.name))\n",
        "\n",
        "premier_data_ref = DataReference(\n",
        "    datastore=ds,\n",
        "    data_reference_name='premier_data',\n",
        "    path_on_datastore=cdh_path)\n",
        "print(\"DataReference object created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intermediate data outputs for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1664901029904
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "data_store = ws.get_default_datastore()\n",
        "flat_features = PipelineData(\"flat_features_data\",datastore=data_store).as_dataset()\n",
        "feature_lookup = PipelineData(\"feature_lookup_data\",datastore=data_store).as_dataset()\n",
        "trimmed_seq = PipelineData(\"trimmed_seq\",datastore=data_store).as_dataset()\n",
        "pat_data = PipelineData(\"pat_data\",datastore=data_store).as_dataset()\n",
        "demog_dict = PipelineData(\"demog_dict_data\",datastore=data_store).as_dataset()\n",
        "all_ftrs_dict = PipelineData(\"all_ftrs_dict_data\",datastore=data_store).as_dataset()\n",
        "int_seqs = PipelineData(\"int_seqs_data\",datastore=data_store).as_dataset()\n",
        "\n",
        "trimmed_seq_pkl = PipelineData(\"trimmed_seq_pkl_data\",datastore=data_store).as_dataset()\n",
        "cohort = PipelineData(\"cohort\",datastore=data_store).as_dataset()\n",
        "model_file = PipelineData(\"model_probs\",datastore=data_store).as_dataset()\n",
        "stats_file = PipelineData(\"analysis_data\",datastore=data_store).as_dataset()\n",
        "preds_file = PipelineData(\"prediction_data\",datastore=data_store).as_dataset()\n",
        "probs_file = PipelineData(\"probs_data\",datastore=data_store).as_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = PipelineParameter(name=\"model_name\", default_value=\"dan\")\n",
        "outcome = PipelineParameter(name=\"outcome\", default_value=\"misa_pt\")\n",
        "n_epochs = PipelineParameter(name=\"n_epochs\",default_value=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 01 - Feature Extraction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1664901031973
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step1 feature_extraction created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step1 = PythonScriptStep(name=\"feature_extraction\",\n",
        "                         script_name=\"feature_extraction.py\", \n",
        "                         inputs=[premier_data_ref.as_download()],\n",
        "                         arguments=[\"--flat_features\",flat_features,\"--feature_lookup\",feature_lookup],\n",
        "                         outputs=[flat_features,feature_lookup],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step1 feature_extraction created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 02 -  Feature Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1664901032956
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step2 feature_tokenization created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step2 = PythonScriptStep(name=\"feature_tokenization\",\n",
        "                         script_name=\"feature_tokenization.py\", \n",
        "                         inputs=[flat_features],\n",
        "                         arguments=[\"--flat_features\",flat_features,\n",
        "                                    \"--trimmed_seq_file\",trimmed_seq,\n",
        "                                    \"--pat_data_file\",pat_data,\n",
        "                                    \"--demog_dict_file\",demog_dict,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--int_seqs_file\",int_seqs],\n",
        "                         outputs=[trimmed_seq,demog_dict,pat_data,all_ftrs_dict,int_seqs],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step2 feature_tokenization created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 03 - Sequence Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1664901035015
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step3 sequence_trimming created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step3 = PythonScriptStep(name=\"sequence_trimming\",\n",
        "                         script_name=\"sequence_trimming.py\", \n",
        "                         inputs=[pat_data,all_ftrs_dict,int_seqs,feature_lookup],\n",
        "                         arguments=[\"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\n",
        "                                    \"--pat_data_file\",pat_data,\n",
        "                                    \"--feature_lookup\",feature_lookup,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--int_seqs_file\",int_seqs,\n",
        "                                    \"--cohort\",cohort],\n",
        "                         outputs=[trimmed_seq_pkl,cohort],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         runconfig=run_config_feature,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step3 sequence_trimming created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 04 - Model Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1664901097034
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step4 train_model created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./training'\n",
        "step4= PythonScriptStep(name=\"train_model\",\n",
        "                         script_name=\"train_model_step.py\", \n",
        "                         inputs=[demog_dict,trimmed_seq_pkl,all_ftrs_dict,feature_lookup,cohort],\n",
        "                         arguments=[\"--model\",model_name,\n",
        "                                    \"--outcome\",outcome,\n",
        "                                    \"--epochs\",n_epochs,\n",
        "                                    \"--model_file\",model_file,\n",
        "                                    \"--stats_file\",stats_file,\n",
        "                                    \"--preds_file\",preds_file, \n",
        "                                    \"--probs_file\",probs_file,   \n",
        "                                    \"--demog_dict_file\",demog_dict,\n",
        "                                    \"--trimmed_seq_pkl_file\",trimmed_seq_pkl,\n",
        "                                    \"--feature_lookup\",feature_lookup,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                    \"--cohort\",cohort],\n",
        "                         outputs=[model_file,stats_file,preds_file,probs_file],\n",
        "                         compute_target=aml_cluster_gpu, \n",
        "                         runconfig=run_config_train,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step4 train_model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Step 05 -  Register model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1664904048017
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step5 register_model created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./register'\n",
        "step5= PythonScriptStep(name=\"register_model\",\n",
        "                         script_name=\"register_model.py\", \n",
        "                         inputs=[model_file,demog_dict,all_ftrs_dict],\n",
        "                         arguments=[\"--model_file\",model_file,\n",
        "                                    \"--demog_dict_file\",demog_dict,\n",
        "                                    \"--all_ftrs_dict_file\",all_ftrs_dict,\n",
        "                                     \"--model\",model_name,\n",
        "                                     \"--outcome\",outcome],\n",
        "                         compute_target=aml_cluster_gpu, \n",
        "                         runconfig=run_config_train,\n",
        "                         source_directory=source_directory,\n",
        "                         allow_reuse=True)\n",
        "print(\"Step5 register_model created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1664904052917
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "steps = [step1,step2,step3,step4,step5]\n",
        "train_pipeline = Pipeline(workspace=ws,steps=steps,default_datastore=data_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Publishing the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating pipeline...\n",
            "Step feature_extraction is ready to be created [b882b76e]\n",
            "Step feature_tokenization is ready to be created [fdcec573]\n",
            "Step sequence_trimming is ready to be created [716cffd0]\n",
            "Step train_model is ready to be created [93e30871]\n",
            "Publishing pipeline...\n",
            "Created step feature_extraction [b882b76e][ccab79d2-d75e-4504-9cd9-782783ad2d7d], (This step will run and generate new outputs)\n",
            "Created step feature_tokenization [fdcec573][4f9f539a-99fd-462d-bc29-f84d211215d2], (This step will run and generate new outputs)\n",
            "Created step sequence_trimming [716cffd0][8e853f83-50a0-48e5-bcbe-3c58acd28921], (This step will run and generate new outputs)\n",
            "Created step train_model [93e30871][03b37fc7-76d9-4662-8f27-7c32c1a38741], (This step will run and generate new outputs)\n",
            "Created step register_model [8738fbf2][3aa0caae-9c1c-4500-9c5f-adec3cc0849b], (This step is eligible to reuse a previous run's output)\n",
            "Using data reference premier_data for StepId [e019594f][b2d198f2-4469-4899-9ebe-e6a54440d08a], (Consumers of this data are eligible to reuse prior runs.)\n",
            "Published pipeline: Job-Pipeline-train-deepmodels\n"
          ]
        }
      ],
      "source": [
        "print(\"Validating pipeline...\")\n",
        "train_pipeline.validate()\n",
        "\n",
        "\n",
        "pipelinename = f\"Job-Pipeline-train-deepmodels\"\n",
        "\n",
        "print(\"Publishing pipeline...\")\n",
        "published_pipeline = train_pipeline.publish(\n",
        "        name=pipelinename,\n",
        "        description=\"Model training/retraining pipeline for the LSTM and DAN Models\",\n",
        "        version=1 )\n",
        "\n",
        "print(f\"Published pipeline: {published_pipeline.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Submit the published pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1664904126089
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitted PipelineRun 2d58824e-0f09-4842-afa7-128e095d39b8\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Job-Pipeline-train-deepmodels/runs/2d58824e-0f09-4842-afa7-128e095d39b8?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/CSELS-CDH-DEV/workspaces/cdh-azml-dev-mlw\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Job-Pipeline-train-deepmodels</td><td>2d58824e-0f09-4842-afa7-128e095d39b8</td><td>azureml.PipelineRun</td><td>Running</td><td><a href=\"https://ml.azure.com/experiments/Job-Pipeline-train-deepmodels/runs/2d58824e-0f09-4842-afa7-128e095d39b8?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/CSELS-CDH-DEV/workspaces/cdh-azml-dev-mlw\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: Job-Pipeline-train-deepmodels,\n",
              "Id: 2d58824e-0f09-4842-afa7-128e095d39b8,\n",
              "Type: azureml.PipelineRun,\n",
              "Status: Running)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL = 'dan'\n",
        "OUTCOME = 'death'\n",
        "EPOCHS = 20\n",
        "\n",
        "run_exp = Experiment(workspace=ws, name=pipelinename)\n",
        "run_exp.submit(published_pipeline,pipeline_parameters={\"model_name\": MODEL,\n",
        "                                            \"outcome\":OUTCOME,\n",
        "                                            \"n_epochs\": EPOCHS})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('azureml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "c29d249f6248e1876db3a43ab8bde2e53ec2cf908dd5eb31493a2d1f2322e9b6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
