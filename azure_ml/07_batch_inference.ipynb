{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Inference for DAN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1664907163954
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing interactive authentication. Please follow the instructions on the terminal.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive authentication successfully completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from azureml.core import  Workspace\n",
        "from azureml.core import Model\n",
        "from azureml.core.resource_configuration import ResourceConfiguration\n",
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "from azureml.core.environment import Environment \n",
        "from azureml.core.webservice import AciWebservice,Webservice\n",
        "from azureml.core.model import Model,InferenceConfig\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE, DEFAULT_GPU_IMAGE\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import Workspace, Experiment, Run, RunConfiguration,ScriptRunConfig\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from tensorflow import keras\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "\n",
        "\n",
        "ws = Workspace.from_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1664907166069
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Workspace name: cdh-azml-dev-mlw\n",
            "Azure region: eastus\n",
            "Subscription id: 320d8d57-c87c-4434-827f-59ee7d86687a\n",
            "Resource group: CSELS-CDH-DEV\n"
          ]
        }
      ],
      "source": [
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create CPU Compute for feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardD13v2'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_compute_cpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS13_V2',\n",
        "                                                           max_nodes=2)\n",
        "    aml_compute_cpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_compute_cpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create GPU Compute for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Find the existing cluster\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "source": [
        "clustername = 'StandardNC6'\n",
        "is_new_cluster = False\n",
        "try:\n",
        "    aml_cluster_gpu = ComputeTarget(workspace = ws,name= clustername)\n",
        "    print(\"Find the existing cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Cluster not find - Creating cluster.....\")\n",
        "    is_new_cluster = True\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='StandardNC6',\n",
        "                                                           max_nodes=2)\n",
        "    aml_cluster_gpu = ComputeTarget.create(ws, clustername, compute_config)\n",
        "\n",
        "aml_cluster_gpu.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Score scipt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%writefile ./score/score.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "import pickle as pkl\n",
        "\n",
        "def init():\n",
        "    global model, model_vocab, model_vocab_demog,N_VOCAB,N_DEMOG\n",
        "    \n",
        "    model_path =  Model.get_model_path(model_name='dan_d1_icu',version=1)\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "def run(input_data):\n",
        "\n",
        "    resultList = []\n",
        "\n",
        "   \n",
        "    num_rows, num_cols = input_data.shape\n",
        "    print(\"input data shape:\",input_data.shape)\n",
        "    print(\"input data type:\",type(input_data))\n",
        "    print(input_data)\n",
        "    #Read comma-delimited data into an array\n",
        "    # data = np.expand_dims(input_data,axis=0)\n",
        "    # Reshape into a 2-dimensional array for model input\n",
        "    prediction = model.predict(input_data).reshape((num_rows, 1))\n",
        "    print(\"prediction shape:\", prediction.shape)\n",
        "    # Append prediction to results\n",
        "    resultList.append(prediction)\n",
        "\n",
        "    return pd.DataFrame(prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Feature processing script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./score/feature_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./score/feature_preprocessing.py\n",
        "import os\n",
        "import time\n",
        "from importlib import reload\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from azureml.core import Model,Dataset\n",
        "import joblib\n",
        "import pickle as pkl\n",
        "from azureml.core import Workspace, Experiment, Run, RunConfiguration\n",
        "\n",
        "def preprocessing(inputs):\n",
        "\n",
        "    run = Run.get_context()\n",
        "    print(\"run name:\",run.display_name)\n",
        "    print(\"run details:\",run.get_details())\n",
        "    \n",
        "    ws = run.experiment.workspace\n",
        "        \n",
        "    #model_vocab_path = Model.get_model_path(model_name='vocab_lstm_icu',_workspace=ws)\n",
        "    #model_demog_vocab_path = model_vocab = Model.get_model_path(model_name='vocab_demog_lstm_icu',_workspace=ws)\n",
        "    data_store = ws.get_default_datastore()\n",
        "    \n",
        "    vocab = Dataset.File.from_files(path=data_store.path('output/pkl/all_ftrs_dict.pkl'))\n",
        "    demog_dict = Dataset.File.from_files(path=data_store.path('output/pkl/demog_dict.pkl'))\n",
        "    \n",
        "    pwd = os.path.dirname(__file__)\n",
        "    output_dir = os.path.abspath(os.path.join(pwd,\"output\"))\n",
        "    pkl_dir = os.path.join(output_dir, \"pkl\")\n",
        "\n",
        "    os.makedirs(pkl_dir, exist_ok=True)\n",
        "    vocab.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
        "    demog_dict.download(target_path=pkl_dir,overwrite=True,ignore_not_found=True)\n",
        "\n",
        "\n",
        "    with open(inputs, \"rb\") as f:\n",
        "        inputs = pkl.load(f)\n",
        "\n",
        "    with open(os.path.join(pkl_dir, \"all_ftrs_dict.pkl\"), \"rb\") as f:\n",
        "        model_vocab = pkl.load(f)\n",
        "\n",
        "    with open(os.path.join(pkl_dir, \"demog_dict.pkl\"), \"rb\") as f:\n",
        "        model_vocab_demog = pkl.load(f)\n",
        "\n",
        "    features = [l[0][-1] for l in inputs]\n",
        "    N_VOCAB = len(model_vocab) + 1\n",
        "    N_DEMOG = len(model_vocab_demog) + 1\n",
        "    print(N_VOCAB,N_DEMOG)\n",
        "    new_demog = [[i + N_VOCAB - 1 for i in l[1]] for l in inputs]\n",
        "    features = [\n",
        "                    features[i] + new_demog[i] for i in range(len(features))\n",
        "                ]\n",
        "    demog_vocab = {k: v + N_VOCAB - 1 for k, v in model_vocab_demog.items()}\n",
        "    model_vocab.update(demog_vocab)\n",
        "    N_VOCAB = np.max([np.max(l) for l in features]) + 1\n",
        "    print(N_VOCAB,N_DEMOG)\n",
        "    X = keras.preprocessing.sequence.pad_sequences(features,padding='post')\n",
        "    print(X.shape)\n",
        "\n",
        "    return X\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(\"feature\")\n",
        "    parser.add_argument(\"--inputs\",type=str)\n",
        "    parser.add_argument(\"--features_file\",type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    inputs = args.inputs\n",
        "    print(inputs)\n",
        "    print(args.features_file)\n",
        "\n",
        "    features = preprocessing(inputs)\n",
        "    #with open(os.path.join(args.features_file,\"features.pkl\"), \"wb\") as f:\n",
        "    # pkl.dump(features, f)\n",
        "    \n",
        "    os.makedirs(args.features_file,exist_ok=True)\n",
        "    np.savetxt(os.path.join(args.features_file,\"features.csv\"), features, delimiter=\",\")\n",
        "    run = Run.get_context()\n",
        "    ws = run.experiment.workspace\n",
        "    \n",
        "    data_store = ws.get_default_datastore()\n",
        "    data_store.upload(src_dir=args.features_file,target_path=args.features_file,overwrite=True,show_progress=True)\n",
        "    datastore_paths = [(data_store, args.features_file)]\n",
        "    inputs = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
        "    inputs.register(name='premier_features',workspace=ws,create_new_version=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reference to input and output data in the datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dataset from Datastore\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Workspace, Dataset\n",
        "from azureml.core import Run\n",
        "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "data_store = ws.get_default_datastore()\n",
        "\n",
        "##########Loading the data from datastore\n",
        "print(\"Creating dataset from Datastore\")\n",
        "inputs = Dataset.File.from_files(path=data_store.path('output/pkl/trimmed_seqs.pkl'))\n",
        "features_file = OutputFileDatasetConfig(destination=(data_store, 'output/csv'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Environment for feature processing and  batch inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "premier_score_model_env = Environment.from_conda_specification(name='premier_score_model_env', file_path='./environments/conda_dependencies_score.yml')\n",
        "# Specify a CPU base image\n",
        "# premier_train_model_env.docker.enabled = True\n",
        "premier_score_model_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "premier_score_model_env.register(workspace=ws)\n",
        "run_config_feature = RunConfiguration()\n",
        "run_config_feature.environment = premier_score_model_env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Preprocessing job for batch inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "job_feature_processing created\n"
          ]
        }
      ],
      "source": [
        "source_directory ='./score'\n",
        "job_feature_processing = ScriptRunConfig(\n",
        "                         script=\"feature_preprocessing.py\", \n",
        "                         arguments=[\"--inputs\",inputs.as_mount(),\"--features_file\",features_file],\n",
        "                         compute_target=aml_compute_cpu, \n",
        "                         environment=premier_score_model_env,\n",
        "                         source_directory=source_directory)\n",
        "print(\"job_feature_processing created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submit Experiment: Job-feature-preprocess-inference\n"
          ]
        }
      ],
      "source": [
        "exp_name = f\"Job-feature-preprocess-inference\"\n",
        "print(\"Submit Experiment:\",exp_name)\n",
        "# Create experiment\n",
        "experiment = Experiment(workspace=ws, name = exp_name)\n",
        "run = experiment.submit(job_feature_processing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Pararallel Run step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "OUTCOME = 'icu'\n",
        "\n",
        "# Get the batch dataset for input\n",
        "batch_data_set = ws.datasets['premier_features']\n",
        "\n",
        "# Set the output location\n",
        "output_dir = OutputFileDatasetConfig(name='inferences',destination=(data_store, 'output/inferences'))\n",
        "\n",
        "# Define the parallel run step step configuration\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory='./score',\n",
        "    entry_script=\"score.py\",\n",
        "    mini_batch_size=\"10MB\",\n",
        "    error_threshold=10000,\n",
        "    output_action='append_row',\n",
        "    append_row_file_name=\"file_size_outputs.txt\",\n",
        "    environment=premier_score_model_env,\n",
        "    compute_target=aml_cluster_gpu,\n",
        "    node_count=2)\n",
        "\n",
        "# Create the parallel run step\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input(\"premier_features\")],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Submit parallel batch inference job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created step batch-score [dd5de4d8][b7c40756-33bb-42c7-bc6e-176d439dae33], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun d8845d54-be45-4b01-9230-6d1ea035e0f9\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/d8845d54-be45-4b01-9230-6d1ea035e0f9?wsid=/subscriptions/320d8d57-c87c-4434-827f-59ee7d86687a/resourcegroups/CSELS-CDH-DEV/workspaces/cdh-azml-dev-mlw&tid=9ce70869-60db-44fd-abe8-d2767077fc8f\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "# Run the pipeline as an experiment\n",
        "### WHICH OUTCOME??\n",
        "exp_name = f\"Job-batch-prediction-{OUTCOME}\"\n",
        "pipeline_run = Experiment(ws, exp_name).submit(pipeline)\n",
        "#pipeline_run.wait_for_completion(show_output=False)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "premier_train_model_env"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('premier_score_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc78c87736aeb3cc28398dbcb4d22314bd77bb9dc1f4148cd78e25835521c0d9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
